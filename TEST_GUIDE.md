# 流式输出功能测试指南

## 测试环境准备

### 1. 配置测试参数

编辑 `test_stream.py` 文件，修改以下配置：

```python
config = {
    'interface_format': 'openai',  # 选择要测试的适配器
    'api_key': 'your-api-key-here',  # 替换为实际的API密钥
    'base_url': 'https://api.openai.com/v1',  # 替换为实际的base_url
    'model_name': 'gpt-3.5-turbo',  # 替换为实际的模型名称
    'temperature': 0.7,
    'max_tokens': 1000,
    'timeout': 60
}
```

### 2. 支持的适配器列表

- `deepseek` - DeepSeek API
- `openai` - OpenAI API
- `azure openai` - Azure OpenAI
- `azure ai` - Azure AI Inference
- `ollama` - Ollama
- `ml studio` - ML Studio
- `gemini` - Google Gemini
- `火山引擎` - 火山引擎
- `硅基流动` - 硅基流动
- `阿里云百炼` - 阿里云百炼（使用OpenAI兼容接口）

## 测试步骤

### 阶段一：单元测试

#### 测试1：普通invoke方法

运行测试脚本，测试基本的invoke方法：

```bash
python test_stream.py
```

**预期结果**：
- ✅ 成功创建适配器
- ✅ 成功调用invoke方法
- ✅ 获得完整的响应

#### 测试2：流式invoke_stream方法

**预期结果**：
- ✅ 成功调用invoke_stream方法
- ✅ 实时输出每个token
- ✅ 回调函数正确收集所有token
- ✅ 返回完整的响应

#### 测试3：stream_utils.invoke_with_cleaning_stream

**预期结果**：
- ✅ 自动检测适配器是否支持流式输出
- ✅ 如果支持，使用真正的流式输出
- ✅ 如果不支持，使用假流式输出
- ✅ 正确清理结果（去除```标记）
- ✅ 正确处理重试逻辑

### 阶段二：集成测试

#### 测试4：完整的架构生成流程

1. 启动主程序：

```bash
python main.py
```

2. 在主界面中：
   - 配置LLM参数
   - 输入小说主题和类型
   - 点击"开始生成架构"

3. 观察向导界面：
   - ✅ 显示"正在连接LLM..."提示
   - ✅ 收到第一个token时移除提示
   - ✅ 实时显示生成内容
   - ✅ 文本自动滚动到最新位置
   - ✅ 进度标签正确更新（"正在连接LLM..." → "生成中..." → "生成完成"）

### 阶段三：多适配器测试

#### 测试5：测试所有适配器

依次测试每个适配器：

1. 修改`test_stream.py`中的`interface_format`
2. 修改对应的`api_key`、`base_url`和`model_name`
3. 运行测试

**检查清单**：
- [ ] DeepSeek
- [ ] OpenAI
- [ ] Azure OpenAI
- [ ] Azure AI
- [ ] Ollama
- [ ] ML Studio
- [ ] Gemini
- [ ] 火山引擎
- [ ] 硅基流动
- [ ] 阿里云百炼

### 阶段四：降级测试

#### 测试6：测试降级逻辑

创建一个不支持流式输出的测试适配器：

```python
class NoStreamAdapter(BaseLLMAdapter):
    def invoke(self, prompt: str) -> str:
        return "测试响应"
    # 不实现invoke_stream方法
```

**预期结果**：
- ✅ 自动检测到不支持流式输出
- ✅ 使用假流式输出
- ✅ 分块输出（每10个字符）
- ✅ 延迟0.005秒

### 阶段五：错误处理测试

#### 测试7：测试错误处理

1. 测试无效的API密钥
2. 测试超时情况
3. 测试网络错误
4. 测试空响应

**预期结果**：
- ✅ 正确捕获异常
- ✅ 正确记录日志
- ✅ 正确重试（最多3次）
- ✅ 最终失败时抛出异常

### 阶段六：性能测试

#### 测试8：测试性能

1. 测试流式输出的延迟
2. 测试UI的响应性
3. 测试长时间生成的稳定性

**预期结果**：
- ✅ 流式输出延迟低（<100ms/token）
- ✅ UI保持响应
- ✅ 长时间生成不卡顿
- ✅ 内存使用稳定

## 测试报告模板

### 测试日期：_____
### 测试人员：_____

#### 单元测试结果
- [ ] 测试1：普通invoke方法 - 通过/失败
- [ ] 测试2：流式invoke_stream方法 - 通过/失败
- [ ] 测试3：stream_utils.invoke_with_cleaning_stream - 通过/失败

#### 集成测试结果
- [ ] 测试4：完整的架构生成流程 - 通过/失败

#### 多适配器测试结果
- [ ] DeepSeek - 通过/失败
- [ ] OpenAI - 通过/失败
- [ ] Azure OpenAI - 通过/失败
- [ ] Azure AI - 通过/失败
- [ ] Ollama - 通过/失败
- [ ] ML Studio - 通过/失败
- [ ] Gemini - 通过/失败
- [ ] 火山引擎 - 通过/失败
- [ ] 硅基流动 - 通过/失败
- [ ] 阿里云百炼 - 通过/失败

#### 降级测试结果
- [ ] 测试6：测试降级逻辑 - 通过/失败

#### 错误处理测试结果
- [ ] 测试7：测试错误处理 - 通过/失败

#### 性能测试结果
- [ ] 测试8：测试性能 - 通过/失败

### 发现的问题：
1. 
2. 
3. 

### 建议的改进：
1. 
2. 
3. 

## 常见问题

### Q1: 如何确认是否使用了真正的流式输出？

A: 查看日志输出，如果看到"使用真正的流式输出"则表示使用了真正的流式输出；如果看到"适配器不支持流式输出，使用假流式输出"则表示使用了假流式输出。

### Q2: 流式输出和假流式输出有什么区别？

A: 
- 真正的流式输出：实时接收LLM生成的每个token，延迟低
- 假流式输出：等待完整响应后，分块模拟流式输出，有固定延迟

### Q3: 如何测试降级逻辑？

A: 创建一个不实现`invoke_stream`方法的适配器，或者临时注释掉某个适配器的`invoke_stream`方法。

### Q4: 测试失败怎么办？

A: 
1. 检查API密钥和配置是否正确
2. 查看日志输出，定位错误信息
3. 确认网络连接是否正常
4. 检查依赖包是否正确安装

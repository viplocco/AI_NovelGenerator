# novel_generator/chapter.py
# -*- coding: utf-8 -*-
"""
ç« èŠ‚è‰ç¨¿ç”ŸæˆåŠè·å–å†å²ç« èŠ‚æ–‡æœ¬ã€å½“å‰ç« èŠ‚æ‘˜è¦ç­‰
"""
import os
import json
import logging
import re  # æ·»åŠ reæ¨¡å—å¯¼å…¥
import time  # æ·»åŠ timeæ¨¡å—å¯¼å…¥
from llm_adapters import create_llm_adapter
from prompt_definitions import (
    first_chapter_draft_prompt, 
    next_chapter_draft_prompt, 
    summarize_recent_chapters_prompt,
    knowledge_filter_prompt,
    knowledge_search_prompt
)
from chapter_directory_parser import get_chapter_info_from_blueprint, get_unit_for_chapter
from novel_generator.common import invoke_with_cleaning
from utils import read_file, clear_file_content, save_string_to_txt
from novel_generator.vectorstore_utils import (
    get_relevant_context_from_vector_store,
    load_vector_store  # æ·»åŠ å¯¼å…¥
)

# ============== è§’è‰²çŠ¶æ€æ™ºèƒ½ç­›é€‰åŠŸèƒ½ ==============

def get_relevant_character_state(filepath: str, characters_involved: str, current_chapter: int) -> str:
    """
    æ ¹æ®ç« èŠ‚æ¶‰åŠè§’è‰²ï¼Œæ™ºèƒ½æå–ç›¸å…³è§’è‰²çŠ¶æ€
    
    å‚æ•°:
        filepath: å°è¯´ä¿å­˜è·¯å¾„
        characters_involved: ç« èŠ‚ç›®å½•ä¸­æŒ‡å®šçš„æ ¸å¿ƒäººç‰©ï¼ˆé€—å·åˆ†éš”ï¼‰
        current_chapter: å½“å‰ç« èŠ‚å·
    
    è¿”å›:
        ç­›é€‰åçš„è§’è‰²çŠ¶æ€æ–‡æœ¬
    """
    character_state_file = os.path.join(filepath, "character_state.txt")
    
    if not os.path.exists(character_state_file):
        return "ï¼ˆæ— è§’è‰²çŠ¶æ€ï¼‰"
    
    full_state = read_file(character_state_file)
    
    # å¦‚æœè§’è‰²çŠ¶æ€è¾ƒçŸ­ï¼ˆ<8000å­—ï¼‰ï¼Œç›´æ¥è¿”å›å…¨éƒ¨
    if len(full_state) < 8000:
        return full_state
    
    # å¦‚æœæœªæŒ‡å®šè§’è‰²ï¼Œä½¿ç”¨ç´¢å¼•æå–æ´»è·ƒè§’è‰²
    if not characters_involved or characters_involved.strip() in ["", "æœªæŒ‡å®š", "æ— "]:
        return _extract_active_characters(full_state, filepath, current_chapter)
    
    # è§£ææŒ‡å®šè§’è‰²åˆ—è¡¨ï¼ˆæ”¯æŒä¸­è‹±æ–‡é€—å·ï¼‰
    specified_chars = []
    for c in characters_involved.replace('ï¼Œ', ',').split(','):
        char_name = c.strip()
        if char_name and char_name not in specified_chars:
            specified_chars.append(char_name)
    
    # å°è¯•æå–æŒ‡å®šè§’è‰²çš„çŠ¶æ€
    relevant_state = _extract_character_blocks(full_state, specified_chars)
    
    # å¦‚æœæå–ç»“æœå¤ªçŸ­ï¼ˆ<500å­—ï¼‰ï¼Œå¯èƒ½åŒ¹é…å¤±è´¥ï¼Œè¿”å›æ´»è·ƒè§’è‰²
    if len(relevant_state) < 500:
        return _extract_active_characters(full_state, filepath, current_chapter)
    
    # åœ¨æå–ç»“æœæœ«å°¾æ·»åŠ "æ–°å‡ºåœºè§’è‰²"éƒ¨åˆ†ï¼ˆå¦‚æœåŸçŠ¶æ€ä¸­æœ‰ï¼‰
    new_chars_section = _extract_new_characters_section(full_state)
    if new_chars_section and new_chars_section not in relevant_state:
        relevant_state += "\n\n" + new_chars_section
    
    return relevant_state


def _extract_character_blocks(full_state: str, char_names: list) -> str:
    """
    ä»å®Œæ•´çŠ¶æ€ä¸­æå–æŒ‡å®šè§’è‰²çš„çŠ¶æ€å—
    
    å‚æ•°:
        full_state: å®Œæ•´çš„è§’è‰²çŠ¶æ€æ–‡æœ¬
        char_names: éœ€è¦æå–çš„è§’è‰²ååˆ—è¡¨
    
    è¿”å›:
        æå–çš„è§’è‰²çŠ¶æ€æ–‡æœ¬
    """
    blocks = []
    current_block = []
    capturing = False
    current_char = None
    
    lines = full_state.split('\n')
    
    for i, line in enumerate(lines):
        # æ£€æµ‹è§’è‰²åè¡Œï¼ˆè§’è‰²åå¼€å¤´ + å†’å·ï¼Œä¸”ä¸æ˜¯å±æ€§è¡Œï¼‰
        is_char_header = False
        stripped_line = line.strip()
        
        # è·³è¿‡å±æ€§è¡Œå’Œå­å±æ€§è¡Œ
        if stripped_line.startswith('â”œ') or stripped_line.startswith('â”‚') or stripped_line.startswith('â””'):
            if capturing:
                current_block.append(line)
            continue
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯è§’è‰²æ ‡é¢˜è¡Œ
        for name in char_names:
            if stripped_line == f"{name}ï¼š" or stripped_line == f"{name}:" or stripped_line.startswith(f"{name}ï¼š") or stripped_line.startswith(f"{name}:"):
                is_char_header = True
                # ä¿å­˜ä¹‹å‰çš„å—
                if capturing and current_block:
                    blocks.append('\n'.join(current_block))
                current_block = [line]
                current_char = name
                capturing = True
                break
        
        if not is_char_header:
            if capturing:
                # æ£€æŸ¥æ˜¯å¦æ˜¯å¦ä¸€ä¸ªè§’è‰²çš„å¼€å§‹ï¼ˆéå±æ€§è¡Œä¸”åŒ…å«å†’å·ï¼‰
                if stripped_line and 'ï¼š' in stripped_line or ':' in stripped_line:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°è§’è‰²
                    possible_name = stripped_line.split('ï¼š')[0].split(':')[0].strip()
                    if possible_name and not possible_name.startswith('â”œ') and not possible_name.startswith('â”‚'):
                        # è¿™å¯èƒ½æ˜¯æ–°è§’è‰²çš„å¼€å§‹
                        if possible_name not in char_names:
                            # ç»“æŸå½“å‰æ•è·
                            blocks.append('\n'.join(current_block))
                            capturing = False
                            current_block = []
                            continue
                
                current_block.append(line)
    
    # ä¿å­˜æœ€åä¸€ä¸ªå—
    if capturing and current_block:
        blocks.append('\n'.join(current_block))
    
    return '\n\n'.join(blocks)


def _extract_active_characters(full_state: str, filepath: str, current_chapter: int) -> str:
    """
    æå–æ´»è·ƒè§’è‰²çŠ¶æ€ï¼ˆç”¨äºçŠ¶æ€æ–‡ä»¶è¿‡å¤§æ—¶ï¼‰
    æ´»è·ƒå®šä¹‰ï¼šæœ€è¿‘30ç« å†…å‡ºç°è¿‡çš„è§’è‰²
    
    å‚æ•°:
        full_state: å®Œæ•´çš„è§’è‰²çŠ¶æ€æ–‡æœ¬
        filepath: å°è¯´ä¿å­˜è·¯å¾„
        current_chapter: å½“å‰ç« èŠ‚å·
    
    è¿”å›:
        æ´»è·ƒè§’è‰²çš„çŠ¶æ€æ–‡æœ¬
    """
    index_file = os.path.join(filepath, "character_index.json")
    
    # å¦‚æœæ²¡æœ‰ç´¢å¼•æ–‡ä»¶ï¼Œè¿”å›çŠ¶æ€æ–‡æœ¬çš„å3000å­—ï¼ˆå‡è®¾æœ€è¿‘æ›´æ–°çš„è§’è‰²æ›´ç›¸å…³ï¼‰
    if not os.path.exists(index_file):
        return full_state[-3000:] if len(full_state) > 3000 else full_state
    
    try:
        with open(index_file, 'r', encoding='utf-8') as f:
            index = json.load(f)
        
        # è·å–æ´»è·ƒè§’è‰²åï¼ˆæœ€è¿‘30ç« å†…å‡ºç°è¿‡ï¼‰
        active_threshold = 30
        active_chars = []
        for name, info in index.items():
            last_chapter = info.get('last_chapter', 0)
            if current_chapter - last_chapter <= active_threshold:
                active_chars.append(name)
        
        if not active_chars:
            # æ²¡æœ‰æ´»è·ƒè§’è‰²ï¼Œè¿”å›å…¨éƒ¨
            return full_state
        
        # æå–æ´»è·ƒè§’è‰²çŠ¶æ€
        result = _extract_character_blocks(full_state, active_chars)
        
        # æ·»åŠ æ–°å‡ºåœºè§’è‰²éƒ¨åˆ†
        new_chars_section = _extract_new_characters_section(full_state)
        if new_chars_section and new_chars_section not in result:
            result += "\n\n" + new_chars_section
        
        return result
        
    except Exception as e:
        logging.warning(f"è¯»å–è§’è‰²ç´¢å¼•å¤±è´¥: {e}ï¼Œä½¿ç”¨æˆªå–æ–¹å¼")
        return full_state[-3000:] if len(full_state) > 3000 else full_state


def _extract_new_characters_section(full_state: str) -> str:
    """
    æå–"æ–°å‡ºåœºè§’è‰²"éƒ¨åˆ†
    
    å‚æ•°:
        full_state: å®Œæ•´çš„è§’è‰²çŠ¶æ€æ–‡æœ¬
    
    è¿”å›:
        æ–°å‡ºåœºè§’è‰²éƒ¨åˆ†æ–‡æœ¬ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    markers = ["æ–°å‡ºåœºè§’è‰²ï¼š", "æ–°å‡ºåœºè§’è‰²:", "æ–°è§’è‰²ï¼š", "æ–°è§’è‰²:"]
    
    for marker in markers:
        if marker in full_state:
            idx = full_state.find(marker)
            return full_state[idx:].strip()
    
    return ""

def get_last_n_chapters_text(chapters_dir: str, current_chapter_num: int, n: int = 3) -> list:
    """
    ä»ç›®å½• chapters_dir ä¸­è·å–æœ€è¿‘ n ç« çš„æ–‡æœ¬å†…å®¹ï¼Œè¿”å›æ–‡æœ¬åˆ—è¡¨ã€‚
    """
    texts = []
    start_chap = max(1, current_chapter_num - n)
    for c in range(start_chap, current_chapter_num):
        chap_file = os.path.join(chapters_dir, f"chapter_{c}.txt")
        if os.path.exists(chap_file):
            text = read_file(chap_file).strip()
            texts.append(text)
        else:
            texts.append("")
    return texts

def summarize_recent_chapters(
    interface_format: str,
    api_key: str,
    base_url: str,
    model_name: str,
    temperature: float,
    max_tokens: int,
    chapters_text_list: list,
    novel_number: int,            # æ–°å¢å‚æ•°
    chapter_info: dict,           # æ–°å¢å‚æ•°
    next_chapter_info: dict,      # æ–°å¢å‚æ•°
    timeout: int = 600
) -> str:  # ä¿®æ”¹è¿”å›å€¼ç±»å‹ä¸º strï¼Œä¸å†æ˜¯ tuple
    """
    æ ¹æ®å‰ä¸‰ç« å†…å®¹ç”Ÿæˆå½“å‰ç« èŠ‚çš„ç²¾å‡†æ‘˜è¦ã€‚
    å¦‚æœè§£æå¤±è´¥ï¼Œåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    try:
        combined_text = "\n".join(chapters_text_list).strip()
        if not combined_text:
            return ""
            
        # é™åˆ¶ç»„åˆæ–‡æœ¬é•¿åº¦
        max_combined_length = 4000
        if len(combined_text) > max_combined_length:
            combined_text = combined_text[-max_combined_length:]
            
        llm_adapter = create_llm_adapter(
            interface_format=interface_format,
            base_url=base_url,
            model_name=model_name,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )
        
        # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½æœ‰é»˜è®¤å€¼
        chapter_info = chapter_info or {}
        next_chapter_info = next_chapter_info or {}
        
        prompt = summarize_recent_chapters_prompt.format(
            combined_text=combined_text,
            novel_number=novel_number,
            chapter_title=chapter_info.get("chapter_title", "æœªå‘½å"),
            chapter_role=chapter_info.get("chapter_role", "å¸¸è§„ç« èŠ‚"),
            chapter_purpose=chapter_info.get("chapter_purpose", "å†…å®¹æ¨è¿›"),
            suspense_level=chapter_info.get("suspense_level", "ä¸­ç­‰"),
            foreshadowing=chapter_info.get("foreshadowing", "æ— "),
            plot_twist_level=chapter_info.get("plot_twist_level", "â˜…â˜†â˜†â˜†â˜†"),
            surface_cultivation=chapter_info.get("surface_cultivation", "æœªè®¾å®š"),
            actual_cultivation=chapter_info.get("actual_cultivation", "æœªè®¾å®š"),
            spatial_coordinates=chapter_info.get("scene_location", "æœªè®¾å®š"),
            chapter_summary=chapter_info.get("chapter_summary", ""),
            next_chapter_number=novel_number + 1,
            next_chapter_title=next_chapter_info.get("chapter_title", "ï¼ˆæœªå‘½åï¼‰"),
            next_chapter_role=next_chapter_info.get("chapter_role", "è¿‡æ¸¡ç« èŠ‚"),
            next_chapter_purpose=next_chapter_info.get("chapter_purpose", "æ‰¿ä¸Šå¯ä¸‹"),
            next_chapter_summary=next_chapter_info.get("chapter_summary", "è¡”æ¥è¿‡æ¸¡å†…å®¹"),
            next_chapter_suspense_level=next_chapter_info.get("suspense_level", "ä¸­ç­‰"),
            next_chapter_foreshadowing=next_chapter_info.get("foreshadowing", "æ— ç‰¹æ®Šä¼ç¬”"),
            next_chapter_plot_twist_level=next_chapter_info.get("plot_twist_level", "â˜…â˜†â˜†â˜†â˜†"),
            next_surface_cultivation=next_chapter_info.get("surface_cultivation", "æœªè®¾å®š"),
            next_actual_cultivation=next_chapter_info.get("actual_cultivation", "æœªè®¾å®š"),
            next_spatial_coordinates=next_chapter_info.get("scene_location", "æœªè®¾å®š")
        )
        
        response_text = invoke_with_cleaning(llm_adapter, prompt)
        summary = extract_summary_from_response(response_text)
        
        if not summary:
            logging.warning("Failed to extract summary, using full response")
            return response_text[:2000]  # é™åˆ¶é•¿åº¦
            
        return summary[:2000]  # é™åˆ¶æ‘˜è¦é•¿åº¦
        
    except Exception as e:
        logging.error(f"Error in summarize_recent_chapters: {str(e)}")
        return ""

def extract_summary_from_response(response_text: str) -> str:
    """ä»å“åº”æ–‡æœ¬ä¸­æå–æ‘˜è¦éƒ¨åˆ†"""
    if not response_text:
        return ""
        
    # æŸ¥æ‰¾æ‘˜è¦æ ‡è®°
    summary_markers = [
        "å½“å‰ç« èŠ‚æ‘˜è¦:", 
        "ç« èŠ‚æ‘˜è¦:",
        "æ‘˜è¦:",
        "æœ¬ç« æ‘˜è¦:"
    ]
    
    for marker in summary_markers:
        if (marker in response_text):
            parts = response_text.split(marker, 1)
            if len(parts) > 1:
                return parts[1].strip()
    
    return response_text.strip()

def format_chapter_info(chapter_info: dict) -> str:
    """å°†ç« èŠ‚ä¿¡æ¯å­—å…¸æ ¼å¼åŒ–ä¸ºæ–‡æœ¬"""
    template = """
ç« èŠ‚ç¼–å·ï¼šç¬¬{number}ç« 
ç« èŠ‚æ ‡é¢˜ï¼šã€Š{title}ã€‹
ç« èŠ‚å®šä½ï¼š{role}
æ ¸å¿ƒä½œç”¨ï¼š{purpose}
ä¸»è¦äººç‰©ï¼š{characters}
å…³é”®é“å…·ï¼š{items}
åœºæ™¯åœ°ç‚¹ï¼š{location}
ä¼ç¬”è®¾è®¡ï¼š{foreshadow}
æ‚¬å¿µå¯†åº¦ï¼š{suspense}
è½¬æŠ˜ç¨‹åº¦ï¼š{twist}
ä¸»è§’ä¿®ä¸ºï¼šè¡¨é¢ä¿®ä¸º{surface_cultivation} | å®é™…å®åŠ›{actual_cultivation}
ç« èŠ‚ç®€è¿°ï¼š{summary}
"""
    return template.format(
        number=chapter_info.get('chapter_number', 'æœªçŸ¥'),
        title=chapter_info.get('chapter_title', 'æœªçŸ¥'),
        role=chapter_info.get('chapter_role', 'æœªçŸ¥'),
        purpose=chapter_info.get('chapter_purpose', 'æœªçŸ¥'),
        characters=chapter_info.get('characters_involved', 'æœªæŒ‡å®š'),
        items=chapter_info.get('key_items', 'æœªæŒ‡å®š'),
        location=chapter_info.get('scene_location', 'æœªæŒ‡å®š'),
        foreshadow=chapter_info.get('foreshadowing', 'æ— '),
        suspense=chapter_info.get('suspense_level', 'ä¸€èˆ¬'),
        twist=chapter_info.get('plot_twist_level', 'â˜…â˜†â˜†â˜†â˜†'),
        surface_cultivation=chapter_info.get('surface_cultivation', 'æœªè®¾å®š'),
        actual_cultivation=chapter_info.get('actual_cultivation', 'æœªè®¾å®š'),
        summary=chapter_info.get('chapter_summary', 'æœªæä¾›')
    )

def safe_format(template: str, **kwargs) -> str:
    """å®‰å…¨æ ¼å¼åŒ–å‡½æ•°ï¼Œå¤„ç†åŒ…å«èŠ±æ‹¬å·çš„å†…å®¹
    
    å‚æ•°:
        template: æ¨¡æ¿å­—ç¬¦ä¸²
        **kwargs: è¦æ›¿æ¢çš„é”®å€¼å¯¹
    
    è¿”å›:
        æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
    """
    from string import Formatter
    
    # è½¬ä¹‰æ‰€æœ‰å­—ç¬¦ä¸²å€¼ä¸­çš„èŠ±æ‹¬å·
    safe_kwargs = {}
    for key, value in kwargs.items():
        if isinstance(value, str):
            # å°†{æ›¿æ¢ä¸º{{ï¼Œ}æ›¿æ¢ä¸º}}ï¼Œé¿å…è¢«formatè¯¯è®¤ä¸ºæ˜¯å ä½ç¬¦
            safe_kwargs[key] = value.replace('{', '{{').replace('}', '}}')
        else:
            safe_kwargs[key] = value
    
    try:
        return Formatter().format(template, **safe_kwargs)
    except Exception as e:
        # å¦‚æœæ ¼å¼åŒ–å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶è¿”å›åŸå§‹æ¨¡æ¿
        logging.warning(f"æ ¼å¼åŒ–å¤±è´¥: {str(e)}, ä½¿ç”¨åŸå§‹æ¨¡æ¿")
        return template


def extract_metadata(text: str, tag_name: str) -> str:
    """æå–çŸ¥è¯†åº“å…ƒæ•°æ®ï¼Œæ”¯æŒå¤šç§æ ¼å¼
    
    å‚æ•°:
        text: åŒ…å«å…ƒæ•°æ®çš„æ–‡æœ¬
        tag_name: æ ‡ç­¾åç§°ï¼ˆå¦‚"ç±»å‹"ã€"åˆ†ç±»"ã€"å…³é”®è¯"ï¼‰
    
    è¿”å›:
        æå–çš„å…ƒæ•°æ®å€¼ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    if not text or not tag_name:
        return ""
    
    # æ”¯æŒå¤šç§æ¢è¡Œç¬¦å’Œæ ¼å¼çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    patterns = [
        rf'ã€{tag_name}ã€‘(.+?)[\r\n]+',  # Windowsæ¢è¡Œç¬¦
        rf'ã€{tag_name}ã€‘(.+?)\n',      # Unixæ¢è¡Œç¬¦
        rf'ã€{tag_name}ã€‘(.+?)(?=ã€|$)',  # åˆ°ä¸‹ä¸€ä¸ªæ ‡ç­¾æˆ–ç»“å°¾
        rf'ã€{tag_name}ã€‘(.+?)\s+',      # ä»»æ„ç©ºç™½å­—ç¬¦
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1).strip()
    
    return ""


def parse_search_keywords(response_text: str) -> list:
    """è§£ææ–°ç‰ˆå…³é”®è¯æ ¼å¼ï¼ˆç¤ºä¾‹è¾“å…¥ï¼š'ç§‘æŠ€å…¬å¸Â·æ•°æ®æ³„éœ²\nåœ°ä¸‹å®éªŒå®¤Â·åŸºå› ç¼–è¾‘'ï¼‰
    
    è§£æè§„åˆ™ï¼š
    1. æå–åŒ…å«'Â·'çš„å…³é”®è¯ç»„
    2. ä¿ç•™çŸ¥è¯†åº“åˆ†ç±»åç§°ï¼ˆå¦‚åœºæ™¯æ„å»ºæ¨¡æ¿ã€æ‚¬å¿µè¥é€ æ‰‹æ³•ç­‰ï¼‰
    3. æœ€å¤šè¿”å›5ç»„å…³é”®è¯
    """
    keywords = []
    for line in response_text.strip().split('\n'):
        line = line.strip()
        if 'Â·' in line:
            # ä¿ç•™åŸå§‹æ ¼å¼ï¼Œä¾¿äºåç»­å‘é‡æ£€ç´¢
            keywords.append(line)
        elif any(cat in line for cat in ['åœºæ™¯æ„å»ºæ¨¡æ¿', 'æ‚¬å¿µè¥é€ æ‰‹æ³•', 'å¯¹è¯å†™ä½œæŠ€å·§', 
                                            'è§†è§’åˆ‡æ¢æŠ€å·§', 'æ—¶é—´è·³è·ƒä¸å›å¿†ç©¿æ’',
                                            'å¤šçº¿å¹¶è¿›å†²çªé›†ä¸­çˆ†å‘', 'ä¼ç¬”çš„é•¿çº¿é“ºè®¾',
                                            'ä¸ªäººç‰©å“åŠçŠ¶æ€ç›˜ç‚¹']):
            # å•ç‹¬çš„åˆ†ç±»åç§°ä¹Ÿä½œä¸ºå…³é”®è¯
            keywords.append(line)
    return keywords[:5]  # æœ€å¤šå–5ç»„

def apply_content_rules(texts: list, novel_number: int, chapter_info: dict = None) -> list:
    """åº”ç”¨å†…å®¹å¤„ç†è§„åˆ™
    
    å‚æ•°:
        texts: å¾…å¤„ç†çš„æ–‡æœ¬åˆ—è¡¨
        novel_number: å½“å‰ç« èŠ‚ç¼–å·
        chapter_info: ç« èŠ‚ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«chapter_role, chapter_purposeç­‰å­—æ®µ
    """
    processed = []
    seen_texts = set()  # ç”¨äºå»é‡çš„é›†åˆï¼Œå­˜å‚¨å·²å¤„ç†çš„æ–‡æœ¬å†…å®¹
    
    for text in texts:
        # æå–æ–‡æœ¬æ ¸å¿ƒå†…å®¹ç”¨äºå»é‡ï¼ˆå»é™¤å‰ç¼€æ ‡è®°ï¼‰
        core_text = text
        for prefix in ["[TECHNIQUE] ", "[SETTING] ", "[GENERAL] ", "[SKIP] ", "[MOD40%] ", "[OK] ", "[PRIOR] "]:
            if core_text.startswith(prefix):
                core_text = core_text[len(prefix):]
                break
        
        # ä½¿ç”¨æ ¸å¿ƒæ–‡æœ¬çš„å“ˆå¸Œå€¼è¿›è¡Œå»é‡
        text_hash = hash(core_text.strip()[:200])  # ä½¿ç”¨å‰200å­—ç¬¦è¿›è¡Œå“ˆå¸Œ
        if text_hash in seen_texts:
            continue  # è·³è¿‡é‡å¤å†…å®¹
        seen_texts.add(text_hash)
        
        # æå–å¹¶ä¿ç•™çŸ¥è¯†åº“å…ƒæ•°æ®
        metadata = ""
        category_tag = ""  # ç”¨äºç²¾ç»†åˆ†ç±»æ ‡è®°
        adaptation_score = 0  # é€‚é…åº¦è¯„åˆ†ï¼ˆ1-10åˆ†ï¼‰
        
        # ä½¿ç”¨å®¹é”™å‡½æ•°æå–å…ƒæ•°æ®
        type_value = extract_metadata(text, "ç±»å‹")
        if type_value:
            metadata += f"[ç±»å‹:{type_value}]"
        
        category_value = extract_metadata(text, "åˆ†ç±»")
        if category_value:
            category = category_value
            metadata += f"[åˆ†ç±»:{category}]"
            # æ ¹æ®åˆ†ç±»ç”Ÿæˆç²¾ç»†æ ‡è®°ï¼Œä¾¿äºåç»­å¤„ç†æ—¶è¯†åˆ«å†…å®¹ç±»å‹
            if "åœºæ™¯æ„å»ºæ¨¡æ¿" in category:
                category_tag = "[åœºæ™¯æ„å»ºæ¨¡æ¿]"
            elif "æ‚¬å¿µè¥é€ æ‰‹æ³•" in category:
                category_tag = "[æ‚¬å¿µè¥é€ æ‰‹æ³•]"
            elif "å¯¹è¯å†™ä½œæŠ€å·§" in category:
                category_tag = "[å¯¹è¯å†™ä½œæŠ€å·§]"
            elif "è§†è§’åˆ‡æ¢æŠ€å·§" in category:
                category_tag = "[è§†è§’åˆ‡æ¢æŠ€å·§]"
            elif "æ—¶é—´è·³è·ƒä¸å›å¿†ç©¿æ’" in category:
                category_tag = "[æ—¶é—´è·³è·ƒä¸å›å¿†ç©¿æ’]"
            elif "å¤šçº¿å¹¶è¿›å†²çªé›†ä¸­çˆ†å‘" in category:
                category_tag = "[å¤šçº¿å¹¶è¿›å†²çªé›†ä¸­çˆ†å‘]"
            elif "ä¼ç¬”çš„é•¿çº¿é“ºè®¾" in category:
                category_tag = "[ä¼ç¬”çš„é•¿çº¿é“ºè®¾]"
            elif "ä¸ªäººç‰©å“åŠçŠ¶æ€ç›˜ç‚¹" in category:
                category_tag = "[ä¸ªäººç‰©å“åŠçŠ¶æ€ç›˜ç‚¹]"

        # ä½¿ç”¨å®¹é”™å‡½æ•°æå–å…³é”®è¯
        keywords_value = extract_metadata(text, "å…³é”®è¯")
        if keywords_value:
            keywords = keywords_value
            metadata += f"[å…³é”®è¯:{keywords}]"
        if re.search(r'ç¬¬[\d]+ç« ', text) or re.search(r'chapter_[\d]+', text):
            chap_nums = list(map(int, re.findall(r'\d+', text)))
            recent_chap = max(chap_nums) if chap_nums else 0
            time_distance = novel_number - recent_chap
            
            if time_distance <= 2:
                processed.append(f"{category_tag}{metadata}[SKIP] è·³è¿‡è¿‘ç« å†…å®¹ï¼š{text[:120]}...")
            elif 3 <= time_distance <= 5:
                processed.append(f"{category_tag}{metadata}[MOD40%] {text}ï¼ˆéœ€ä¿®æ”¹â‰¥40%ï¼‰")
            else:
                processed.append(f"{category_tag}{metadata}[OK] {text}ï¼ˆå¯å¼•ç”¨æ ¸å¿ƒï¼‰")
        else:
            processed.append(f"{category_tag}{metadata}[PRIOR] {text}ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰")
    return processed

def apply_knowledge_rules(contexts: list, chapter_num: int) -> list:
    """åº”ç”¨çŸ¥è¯†åº“ä½¿ç”¨è§„åˆ™"""
    processed = []
    for text in contexts:
        # æ£€æµ‹å†å²ç« èŠ‚å†…å®¹
        if "ç¬¬" in text and "ç« " in text:
            # æå–ç« èŠ‚å·åˆ¤æ–­æ—¶é—´è¿œè¿‘
            chap_nums = [int(s) for s in text.split() if s.isdigit()]
            recent_chap = max(chap_nums) if chap_nums else 0
            time_distance = chapter_num - recent_chap
            
            # ç›¸ä¼¼åº¦å¤„ç†è§„åˆ™
            if time_distance <= 3:  # è¿‘ä¸‰ç« å†…å®¹
                processed.append(f"[å†å²ç« èŠ‚é™åˆ¶] è·³è¿‡è¿‘æœŸå†…å®¹: {text[:50]}...")
                continue
                
            # å…è®¸å¼•ç”¨ä½†éœ€è¦è½¬æ¢
            processed.append(f"[å†å²å‚è€ƒ] {text} (éœ€è¿›è¡Œ30%ä»¥ä¸Šæ”¹å†™)")
        else:
            # ç¬¬ä¸‰æ–¹çŸ¥è¯†ä¼˜å…ˆå¤„ç†
            processed.append(f"[å¤–éƒ¨çŸ¥è¯†] {text}")
    return processed

def get_filtered_knowledge_context(
    api_key: str,
    base_url: str,
    model_name: str,
    interface_format: str,
    embedding_adapter,
    filepath: str,
    chapter_info: dict,
    retrieved_texts: list,
    max_tokens: int = 2048,
    timeout: int = 600
) -> str:
    """ä¼˜åŒ–åçš„çŸ¥è¯†è¿‡æ»¤å¤„ç†"""
    if not retrieved_texts:
        return "ï¼ˆæ— ç›¸å…³çŸ¥è¯†åº“å†…å®¹ï¼‰"

    try:
        processed_texts = apply_knowledge_rules(retrieved_texts, chapter_info.get('chapter_number', 0))
        
        # å»é‡å¤„ç†ï¼šåŸºäºæ–‡æœ¬å†…å®¹çš„æ ¸å¿ƒéƒ¨åˆ†
        seen_core_texts = set()
        unique_processed_texts = []
        for text in processed_texts:
            # æå–æ ¸å¿ƒæ–‡æœ¬å†…å®¹ï¼ˆå»é™¤å‰ç¼€æ ‡è®°ï¼‰
            core_text = text
            for prefix in ["[å†å²ç« èŠ‚é™åˆ¶] ", "[å†å²å‚è€ƒ] ", "[å¤–éƒ¨çŸ¥è¯†] ", "[TECHNIQUE] ", "[SETTING] ", "[GENERAL] "]:
                if core_text.startswith(prefix):
                    core_text = core_text[len(prefix):]
                    break
            # ä½¿ç”¨å‰150å­—ç¬¦è¿›è¡Œå»é‡åˆ¤æ–­
            core_hash = hash(core_text.strip()[:150])
            if core_hash not in seen_core_texts:
                seen_core_texts.add(core_hash)
                unique_processed_texts.append(text)
        processed_texts = unique_processed_texts
        
        llm_adapter = create_llm_adapter(
            interface_format=interface_format,
            base_url=base_url,
            model_name=model_name,
            api_key=api_key,
            temperature=0.3,
            max_tokens=max_tokens,
            timeout=timeout
        )
        
        # é™åˆ¶æ£€ç´¢æ–‡æœ¬é•¿åº¦å¹¶æ ¼å¼åŒ–ï¼ŒåŒæ—¶ä¿ç•™çŸ¥è¯†åº“å…ƒæ•°æ®
        formatted_texts = []
        seen_formatted = set()  # æ ¼å¼åŒ–åçš„å»é‡é›†åˆ
        max_text_length = 600
        for i, text in enumerate(processed_texts, 1):
            # æ£€æŸ¥å¹¶ä¿ç•™çŸ¥è¯†åº“å…ƒæ•°æ®
            metadata_prefix = ""
            
            # ä½¿ç”¨å®¹é”™å‡½æ•°æå–å…ƒæ•°æ®
            type_value = extract_metadata(text, "ç±»å‹")
            if type_value:
                metadata_prefix += f"[ç±»å‹:{type_value}]"
            
            category_value = extract_metadata(text, "åˆ†ç±»")
            if category_value:
                metadata_prefix += f"[åˆ†ç±»:{category_value}]"
            
            keywords_value = extract_metadata(text, "å…³é”®è¯")
            if keywords_value:
                metadata_prefix += f"[å…³é”®è¯:{keywords_value}]"
            
            # æˆªå–æ–‡æœ¬å†…å®¹ï¼Œä½†ä¿ç•™å…ƒæ•°æ®
            content_start = text.find("ã€")
            content_start = text.find("ã€") if content_start != -1 else 0
            content_text = text[content_start:]
            
            # ç»„åˆå…ƒæ•°æ®å’Œå†…å®¹
            full_text = f"{metadata_prefix}\n{content_text}" if metadata_prefix else content_text
            
            if len(full_text) > max_text_length:
                # ä¿ç•™å…ƒæ•°æ®ï¼Œæˆªå–å†…å®¹éƒ¨åˆ†
                if len(metadata_prefix) < max_text_length:
                    content_max_len = max_text_length - len(metadata_prefix) - 1
                    full_text = f"{metadata_prefix}\n{content_text[:content_max_len]}..."
                else:
                    full_text = full_text[:max_text_length] + "..."
            
            # æœ€ç»ˆå»é‡æ£€æŸ¥
            formatted_hash = hash(full_text.strip()[:200])
            if formatted_hash in seen_formatted:
                continue
            seen_formatted.add(formatted_hash)
            
            formatted_texts.append(f"[é¢„å¤„ç†ç»“æœ{i}]\n{full_text}")

        # ä½¿ç”¨æ ¼å¼åŒ–å‡½æ•°å¤„ç†ç« èŠ‚ä¿¡æ¯
        formatted_chapter_info = (
            f"å½“å‰ç« èŠ‚å®šä½ï¼š{chapter_info.get('chapter_role', '')}\n"
            f"æ ¸å¿ƒç›®æ ‡ï¼š{chapter_info.get('chapter_purpose', '')}\n"
            f"å…³é”®è¦ç´ ï¼š{chapter_info.get('characters_involved', '')} | "
            f"{chapter_info.get('key_items', '')} | "
            f"{chapter_info.get('scene_location', '')}"
        )
        # æ ¹æ®ç« èŠ‚æ ¸å¿ƒä½œç”¨æ˜ å°„åˆ°æƒ…èŠ‚ç±»å‹
        purpose_mapping = {
            "æ¨è¿›": "å‘å±•",
            "è½¬æŠ˜": "è½¬æŠ˜",
            "æ­ç¤º": "æ­ç¤º",
            "é“ºå«": "é“ºå«",
            "é«˜æ½®": "é«˜æ½®"
        }
        chapter_purpose_value = chapter_info.get('chapter_purpose', '')
        plot_type = purpose_mapping.get(chapter_purpose_value, chapter_purpose_value)

        # æ ¹æ®æ‚¬å¿µå¯†åº¦æ˜ å°„åˆ°å¼ åŠ›çº§åˆ«
        suspense_mapping = {
            "ç´§å‡‘": "é«˜",
            "æ¸è¿›": "ä¸­",
            "çˆ†å‘": "æé«˜",
            "å¹³ç¼“": "ä½"
        }
        suspense_value = chapter_info.get('suspense_level', '')
        tension_level = suspense_mapping.get(suspense_value, suspense_value)


        prompt = safe_format(
            knowledge_filter_prompt,
            chapter_number=chapter_info.get('chapter_number', ''),
            chapter_title=chapter_info.get('chapter_title', ''),
            chapter_role=chapter_info.get('chapter_role', ''),
            chapter_purpose=chapter_info.get('chapter_purpose', ''),
            plot_type=plot_type,
            tension_level=tension_level,
            similarity_threshold="0.7",  # Default similarity threshold
            value_density_requirement="ä¸­ç­‰",  # Default value density requirement
            filter_primary_goal="è·å–ä¸å½“å‰ç« èŠ‚é«˜åº¦ç›¸å…³çš„å†…å®¹",  # Default primary goal
            filter_secondary_goals="è¡¥å……èƒŒæ™¯ä¿¡æ¯ã€æä¾›ç»†èŠ‚æè¿°",  # Default secondary goals
            retrieved_texts="\n\n".join(formatted_texts) if formatted_texts else "ï¼ˆæ— æ£€ç´¢ç»“æœï¼‰"
        )
        
        filtered_content = invoke_with_cleaning(llm_adapter, prompt)
        return filtered_content if filtered_content else "ï¼ˆçŸ¥è¯†å†…å®¹è¿‡æ»¤å¤±è´¥ï¼‰"
        
    except Exception as e:
        logging.error(f"Error in knowledge filtering: {str(e)}")
        return "ï¼ˆå†…å®¹è¿‡æ»¤è¿‡ç¨‹å‡ºé”™ï¼‰"

def build_chapter_prompt(
    api_key: str,
    base_url: str,
    model_name: str,
    filepath: str,
    novel_number: int,
    word_number: int,
    temperature: float,
    user_guidance: str,
    characters_involved: str,
    key_items: str,
    scene_location: str,
    time_constraint: str,
    embedding_api_key: str,
    embedding_url: str,
    embedding_interface_format: str,
    embedding_model_name: str,
    embedding_retrieval_k: int = 2,
    interface_format: str = "openai",
    max_tokens: int = 2048,
    timeout: int = 600,
    prompt_callback: callable = None,
    progress_callback: callable = None
) -> str:
    """
    æ„é€ å½“å‰ç« èŠ‚çš„è¯·æ±‚æç¤ºè¯ï¼ˆå®Œæ•´å®ç°ç‰ˆï¼‰
    ä¿®æ”¹é‡ç‚¹ï¼š
    1. ä¼˜åŒ–çŸ¥è¯†åº“æ£€ç´¢æµç¨‹
    2. æ–°å¢å†…å®¹é‡å¤æ£€æµ‹æœºåˆ¶
    3. é›†æˆæç¤ºè¯åº”ç”¨è§„åˆ™

    å‚æ•°:
        prompt_callback: æç¤ºè¯æ„å»ºè¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶æ–‡æœ¬å‚æ•°
        progress_callback: è¿›åº¦æ›´æ–°å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶(progress, description)å‚æ•°
    """
    # è¯»å–åŸºç¡€æ–‡ä»¶
    if progress_callback:
        progress_callback(0.1, "è¯»å–åŸºç¡€æ–‡ä»¶")
    
    # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
    current_progress = 0.1
    progress_steps = [
        (0.2, "è·å–ç« èŠ‚å’Œå•å…ƒä¿¡æ¯"),
        (0.3, "å‡†å¤‡ç”Ÿæˆç« èŠ‚æ‘˜è¦"),
        (0.4, "æ­£åœ¨ç”Ÿæˆç« èŠ‚æ‘˜è¦"),
        (0.5, "ç”ŸæˆçŸ¥è¯†åº“æ£€ç´¢æç¤ºè¯"),
        (0.6, "æ£€ç´¢çŸ¥è¯†åº“"),
        (0.7, "å¤„ç†çŸ¥è¯†åº“å†…å®¹"),
        (0.8, "è¿‡æ»¤çŸ¥è¯†åº“å†…å®¹"),
        (0.85, "æ„å»ºå®Œæ•´æç¤ºè¯"),
        (0.9, "æç¤ºè¯æ„å»ºå®Œæˆ"),
        (1.0, "å®Œæˆ")
    ]
    current_step = 0

    arch_file = os.path.join(filepath, "Novel_architecture.txt")
    novel_architecture_text = read_file(arch_file)
    directory_file = os.path.join(filepath, "Novel_directory.txt")

    # æ£€æŸ¥ç« èŠ‚ç›®å½•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(directory_file):
        print(f"è­¦å‘Š: ç« èŠ‚ç›®å½•æ–‡ä»¶ä¸å­˜åœ¨: {directory_file}")
        blueprint_text = ""
    else:
        blueprint_text = read_file(directory_file)
        if not blueprint_text.strip():
            print(f"è­¦å‘Š: ç« èŠ‚ç›®å½•æ–‡ä»¶ä¸ºç©º: {directory_file}")
        else:
            pass
    global_summary_file = os.path.join(filepath, "global_summary.txt")
    global_summary_text = read_file(global_summary_file)
    
    # ä½¿ç”¨æ™ºèƒ½è§’è‰²ç­›é€‰åŠŸèƒ½ï¼Œåªè·å–ç›¸å…³è§’è‰²çŠ¶æ€
    # è·å–ç« èŠ‚ä¿¡æ¯ä¸­çš„è§’è‰²ä¿¡æ¯
    temp_chapter_info = get_chapter_info_from_blueprint(blueprint_text if 'blueprint_text' in dir() else read_file(directory_file), novel_number)
    temp_characters = temp_chapter_info.get("characters_involved", characters_involved) if temp_chapter_info else characters_involved
    character_state_text = get_relevant_character_state(filepath, temp_characters, novel_number)
    
    plot_arcs_file = os.path.join(filepath, "plot_arcs.txt")
    plot_arcs_text = ""
    if os.path.exists(plot_arcs_file):
        plot_arcs_text = read_file(plot_arcs_file)
    
    # è·å–å•å…ƒä¿¡æ¯
    unit_info = get_unit_for_chapter(blueprint_text, novel_number)
    
    # æ„å»ºå•å…ƒä¿¡æ¯æ–‡æœ¬
    unit_info_text = ""
    if unit_info:
        # è·å–å•å…ƒæ ‡é¢˜å’Œç« èŠ‚èŒƒå›´
        unit_title = unit_info.get('unit_title', 'æœªçŸ¥')
        start_chapter = unit_info.get('start_chapter', 0)
        end_chapter = unit_info.get('end_chapter', 0)
        
        # å¦‚æœå•å…ƒæ ‡é¢˜ä¸åŒ…å«ç« èŠ‚èŒƒå›´ä¿¡æ¯ï¼Œåˆ™æ·»åŠ 
        if start_chapter > 0 and end_chapter > 0 and "ï¼ˆåŒ…å«ç« èŠ‚" not in unit_title:
            unit_title = f"{unit_title}ï¼ˆåŒ…å«ç« èŠ‚ï¼š{start_chapter}-{end_chapter}ç« ï¼‰"
        
        unit_info_text = f"""
[å•å…ƒä¿¡æ¯]
å•å…ƒæ ‡é¢˜ï¼š{unit_title}
å•å…ƒå®šä½ï¼š{unit_info.get('unit_location', 'æœªçŸ¥')}
æ ¸å¿ƒä½œç”¨ï¼š{unit_info.get('unit_purpose', 'æœªçŸ¥')}
å†…å®¹æ‘˜è¦ï¼š{unit_info.get('unit_summary', 'æœªçŸ¥')}
ä¿®ä¸ºç­‰çº§èŒƒå›´ï¼š{unit_info.get('cultivation_range', 'æœªçŸ¥')}
ç©ºé—´åæ ‡èŒƒå›´ï¼š{unit_info.get('spatial_range', 'æœªçŸ¥')}
æ¨èçš„è·¨ç« èŠ‚å†™ä½œæ‰‹æ³•ï¼š{unit_info.get('recommended_techniques', 'æ— ')}
"""
    else:
        unit_info_text = "\n[å•å…ƒä¿¡æ¯]\nå½“å‰ç« èŠ‚æœªæ‰¾åˆ°æ‰€å±å•å…ƒä¿¡æ¯ã€‚\n"
    
    # è·å–ç« èŠ‚ä¿¡æ¯
    if not blueprint_text or not blueprint_text.strip():
        print(f"é”™è¯¯: ç« èŠ‚ç›®å½•ä¸ºç©ºï¼Œæ— æ³•è·å–ç« èŠ‚ {novel_number} çš„ä¿¡æ¯")
        print(f"æç¤º: è¯·å…ˆç”Ÿæˆç« èŠ‚ç›®å½•ï¼ˆæ­¥éª¤2ï¼‰")
        # æ„å»ºé»˜è®¤æç¤ºè¯
        default_prompt = safe_format(
            next_chapter_draft_prompt,
            user_guidance=user_guidance if user_guidance else "æ— ç‰¹æ®ŠæŒ‡å¯¼",
            global_summary=global_summary_text if global_summary_text else "ï¼ˆæ— å…¨å±€æ‘˜è¦ï¼‰",
            previous_chapter_excerpt="ï¼ˆæ— å‰æ–‡ï¼‰",
            character_state=character_state_text if character_state_text else "ï¼ˆæ— è§’è‰²çŠ¶æ€ï¼‰",
            short_summary="ï¼ˆæ— ç« èŠ‚æ‘˜è¦ï¼‰",
            novel_number=novel_number,
            chapter_title=f"ç¬¬{novel_number}ç« ",
            chapter_role="æœªè®¾å®š",
            chapter_purpose="æœªè®¾å®š",
            suspense_level="æœªè®¾å®š",
            foreshadowing="æœªè®¾å®š",
            plot_twist_level="â˜…â˜†â˜†â˜†â˜†",
            chapter_summary="æœªè®¾å®š",
            word_number=word_number,
            characters_involved=characters_involved if characters_involved else "æœªæŒ‡å®š",
            key_items=key_items if key_items else "æœªæŒ‡å®š",
            scene_location=scene_location if scene_location else "æœªè®¾å®š",
            time_constraint=time_constraint if time_constraint else "æœªè®¾å®š",
            next_chapter_number=novel_number + 1,
            next_chapter_title=f"ç¬¬{novel_number + 1}ç« ",
            next_chapter_role="æœªè®¾å®š",
            next_chapter_purpose="æœªè®¾å®š",
            next_chapter_suspense_level="æœªè®¾å®š",
            next_chapter_foreshadowing="æœªè®¾å®š",
            next_chapter_plot_twist_level="â˜…â˜†â˜†â˜†â˜†",
            next_chapter_summary="æœªè®¾å®š",
            filtered_context="ï¼ˆæ— çŸ¥è¯†åº“å†…å®¹ï¼‰",
            unit_info=unit_info_text
        )
        # è°ƒç”¨å›è°ƒå‡½æ•°æ˜¾ç¤ºæç¤ºè¯å†…å®¹
        if prompt_callback:
            prompt_callback(f"\n[å®Œæ•´æç¤ºè¯]\n{default_prompt}")
        if progress_callback:
            progress_callback(1.0, "ç« èŠ‚ç›®å½•ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯")
        return default_prompt

    chapter_info = get_chapter_info_from_blueprint(blueprint_text, novel_number)

    if progress_callback:
        current_step += 1
        progress_callback(progress_steps[current_step][0], progress_steps[current_step][1])

    chapter_title = chapter_info.get("chapter_title", f"ç¬¬{novel_number}ç« ")
    chapter_role = chapter_info.get("chapter_role", "æœªè®¾å®š")
    chapter_purpose = chapter_info.get("chapter_purpose", "æœªè®¾å®š")
    suspense_level = chapter_info.get("suspense_level", "æœªè®¾å®š")
    foreshadowing = chapter_info.get("foreshadowing", "æœªè®¾å®š")
    plot_twist_level = chapter_info.get("plot_twist_level", "â˜…â˜†â˜†â˜†â˜†")
    surface_cultivation = chapter_info.get("surface_cultivation", "æœªè®¾å®š")
    actual_cultivation = chapter_info.get("actual_cultivation", "æœªè®¾å®š")
    scene_location = chapter_info.get("scene_location", "æœªè®¾å®š")
    chapter_summary = chapter_info.get("chapter_summary", "æœªè®¾å®š")

    # è·å–ä¸‹ä¸€ç« èŠ‚ä¿¡æ¯
    next_chapter_number = novel_number + 1
    next_chapter_info = get_chapter_info_from_blueprint(blueprint_text, next_chapter_number)
    next_chapter_title = next_chapter_info.get("chapter_title", "ï¼ˆæœªå‘½åï¼‰")
    next_chapter_role = next_chapter_info.get("chapter_role", "è¿‡æ¸¡ç« èŠ‚")
    next_chapter_purpose = next_chapter_info.get("chapter_purpose", "æ‰¿ä¸Šå¯ä¸‹")
    next_chapter_suspense = next_chapter_info.get("suspense_level", "ä¸­ç­‰")
    next_chapter_foreshadow = next_chapter_info.get("foreshadowing", "æ— ç‰¹æ®Šä¼ç¬”")
    next_chapter_twist = next_chapter_info.get("plot_twist_level", "â˜…â˜†â˜†â˜†â˜†")
    next_surface_cultivation = next_chapter_info.get("surface_cultivation", "æœªè®¾å®š")
    next_actual_cultivation = next_chapter_info.get("actual_cultivation", "æœªè®¾å®š")
    next_scene_location = next_chapter_info.get("scene_location", "æœªè®¾å®š")
    next_chapter_summary = next_chapter_info.get("chapter_summary", "è¡”æ¥è¿‡æ¸¡å†…å®¹")

    # åˆ›å»ºç« èŠ‚ç›®å½•
    chapters_dir = os.path.join(filepath, "chapters")
    os.makedirs(chapters_dir, exist_ok=True)

    # ç¬¬ä¸€ç« ç‰¹æ®Šå¤„ç†
    if novel_number == 1:
        first_prompt = safe_format(
            first_chapter_draft_prompt,
            novel_number=novel_number,
            word_number=word_number,
            chapter_title=chapter_title,
            unit_info=unit_info_text,
            chapter_role=chapter_role,
            chapter_purpose=chapter_purpose,
            suspense_level=suspense_level,
            foreshadowing=foreshadowing,
            plot_arcs=plot_arcs_text if plot_arcs_text else "ï¼ˆæ— å‰§æƒ…è¦ç‚¹ï¼‰",
            plot_twist_level=plot_twist_level,
            surface_cultivation=surface_cultivation,
            actual_cultivation=actual_cultivation,
            chapter_summary=chapter_summary,
            characters_involved=characters_involved,
            key_items=key_items,
            scene_location=scene_location,
            time_constraint=time_constraint,
            user_guidance=user_guidance,
            novel_setting=novel_architecture_text,
            filtered_context="ï¼ˆæ— çŸ¥è¯†åº“å†…å®¹ï¼‰"
        )
        # è°ƒç”¨å›è°ƒå‡½æ•°æ˜¾ç¤ºæç¤ºè¯å†…å®¹
        if prompt_callback:
            prompt_callback(f"\n[å®Œæ•´æç¤ºè¯]\n{first_prompt}")
        if progress_callback:
            progress_callback(1.0, "æç¤ºè¯æ„å»ºå®Œæˆ")
        return first_prompt

    # è·å–å‰æ–‡å†…å®¹å’Œæ‘˜è¦
    if progress_callback:
        current_step += 1
        progress_callback(progress_steps[current_step][0], progress_steps[current_step][1])

    recent_texts = get_last_n_chapters_text(chapters_dir, novel_number, n=3)
    
    try:
        if progress_callback:
            current_step += 1
            progress_callback(progress_steps[current_step][0], progress_steps[current_step][1])

        logging.info("Attempting to generate summary")
        short_summary = summarize_recent_chapters(
            interface_format=interface_format,
            api_key=api_key,
            base_url=base_url,
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            chapters_text_list=recent_texts,
            novel_number=novel_number,
            chapter_info=chapter_info,
            next_chapter_info=next_chapter_info,
            timeout=timeout
        )
        logging.info("Summary generated successfully")

        # æ·»åŠ å»¶æ—¶ï¼Œè®©ç”¨æˆ·èƒ½çœ‹åˆ°è¿›åº¦å˜åŒ–
        time.sleep(1)
    except Exception as e:
        logging.error(f"Error in summarize_recent_chapters: {str(e)}")
        short_summary = "ï¼ˆæ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼‰"

    # è·å–å‰ä¸€ç« ç»“å°¾
    previous_excerpt = ""
    for text in reversed(recent_texts):
        if text.strip():
            previous_excerpt = text[-800:] if len(text) > 800 else text
            break

    # çŸ¥è¯†åº“æ£€ç´¢å’Œå¤„ç†
    if progress_callback:
        current_step += 1
        progress_callback(progress_steps[current_step][0], progress_steps[current_step][1])

    try:
        # ç”Ÿæˆæ£€ç´¢å…³é”®è¯
        if progress_callback:
            current_step += 1
            progress_callback(progress_steps[current_step][0], progress_steps[current_step][1])
        llm_adapter = create_llm_adapter(
            interface_format=interface_format,
            base_url=base_url,
            model_name=model_name,
            api_key=api_key,
            temperature=0.3,
            max_tokens=max_tokens,
            timeout=timeout
        )
        
        search_prompt = safe_format(
            knowledge_search_prompt,
            chapter_number=novel_number,
            chapter_title=chapter_title,
            chapter_role=chapter_role,
            chapter_purpose=chapter_purpose,
            plot_type=chapter_purpose,  # Using chapter_purpose as plot_type
            tension_level=suspense_level,  # Using suspense_level as tension_level
            plot_focus=chapter_role,  # Using chapter_role as plot_focus
            foreshadowing_type=foreshadowing,
            main_characters=characters_involved,
            character_states="",  # Not available in current context
            scene_location=scene_location,
            scene_features="",  # Not available in current context
            time_setting=time_constraint,
            atmosphere="",  # Not available in current context
            key_items=key_items,
            related_technology="",  # Not available in current context
            previous_summary="",  # Not available in current context
            current_summary=short_summary,
            future_expectations="",  # Not available in current context
            user_guidance=user_guidance
        )
        
        search_response = invoke_with_cleaning(llm_adapter, search_prompt)
        keyword_groups = parse_search_keywords(search_response)
        
        # æ·»åŠ å•å…ƒæ¨èçš„å†™ä½œæ‰‹æ³•ä½œä¸ºé¢å¤–æ£€ç´¢å…³é”®è¯ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
        if unit_info and unit_info.get('recommended_techniques'):
            techniques = unit_info['recommended_techniques']
            # å¤„ç†å¤šç§åˆ†éš”ç¬¦ï¼šé€—å·ã€åˆ†å·ã€é¡¿å·ã€ç©ºæ ¼
            import re
            tech_list = re.split(r'[,ï¼Œ;ï¼›ã€\s]+', techniques)
            for tech in tech_list:
                tech = tech.strip()
                if tech and tech not in keyword_groups:
                    # å°†å•å…ƒæ¨èçš„æŠ€æ³•ä½œä¸ºé«˜ä¼˜å…ˆçº§å…³é”®è¯ï¼ˆåœ¨å‰é¢æ·»åŠ ï¼‰
                    keyword_groups.insert(0, f"[å•å…ƒæŠ€æ³•] {tech}")

        # æ‰§è¡Œå‘é‡æ£€ç´¢
        all_contexts = []
        seen_contexts = set()  # ç”¨äºå»é‡çš„é›†åˆ
        from embedding_adapters import create_embedding_adapter
        embedding_adapter = create_embedding_adapter(
            embedding_interface_format,
            embedding_api_key,
            embedding_url,
            embedding_model_name
        )
        
        store = load_vector_store(embedding_adapter, filepath)
        if store:
            collection_size = store._collection.count()
            actual_k = min(embedding_retrieval_k, max(1, collection_size))
            
            for group in keyword_groups:
                context = get_relevant_context_from_vector_store(
                    embedding_adapter=embedding_adapter,
                    query=group,
                    filepath=filepath,
                    k=actual_k
                )
                if context:
                    # ä½¿ç”¨å†…å®¹çš„å“ˆå¸Œå€¼è¿›è¡Œå»é‡
                    context_hash = hash(context.strip())
                    if context_hash in seen_contexts:
                        continue  # è·³è¿‡é‡å¤å†…å®¹
                    seen_contexts.add(context_hash)
                    
                    if any(kw in group.lower() for kw in ["æŠ€æ³•", "æ‰‹æ³•", "æ¨¡æ¿"]):
                        all_contexts.append(f"[TECHNIQUE] {context}")
                    elif any(kw in group.lower() for kw in ["è®¾å®š", "æŠ€æœ¯", "ä¸–ç•Œè§‚"]):
                        all_contexts.append(f"[SETTING] {context}")
                    else:
                        all_contexts.append(f"[GENERAL] {context}")

        # åº”ç”¨å†…å®¹è§„åˆ™
        # å…ˆæ„å»ºchapter_infoå­—å…¸
        chapter_info_for_rules = {
            "chapter_number": novel_number,
            "chapter_title": chapter_title,
            "chapter_role": chapter_role,
            "chapter_purpose": chapter_purpose,
            "characters_involved": characters_involved,
            "key_items": key_items,
            "scene_location": scene_location,
            "foreshadowing": foreshadowing,
            "suspense_level": suspense_level,
            "plot_twist_level": plot_twist_level,
            "surface_cultivation": surface_cultivation,
            "actual_cultivation": actual_cultivation,
            "chapter_summary": chapter_summary,
            "time_constraint": time_constraint
        }
        processed_contexts = apply_content_rules(all_contexts, novel_number, chapter_info_for_rules)
        
        # æ‰§è¡ŒçŸ¥è¯†è¿‡æ»¤
        chapter_info_for_filter = {
            "chapter_number": novel_number,
            "chapter_title": chapter_title,
            "chapter_role": chapter_role,
            "chapter_purpose": chapter_purpose,
            "characters_involved": characters_involved,
            "key_items": key_items,
            "scene_location": scene_location,
            "foreshadowing": foreshadowing,  # ä¿®å¤æ‹¼å†™é”™è¯¯
            "suspense_level": suspense_level,
            "plot_twist_level": plot_twist_level,
            "surface_cultivation": surface_cultivation,
            "actual_cultivation": actual_cultivation,
            "chapter_summary": chapter_summary,
            "time_constraint": time_constraint
        }
        
        filtered_context = get_filtered_knowledge_context(
            api_key=api_key,
            base_url=base_url,
            model_name=model_name,
            interface_format=interface_format,
            embedding_adapter=embedding_adapter,
            filepath=filepath,
            chapter_info=chapter_info_for_filter,
            retrieved_texts=processed_contexts,
            max_tokens=max_tokens,
            timeout=timeout
        )
        
    except Exception as e:
        logging.error(f"çŸ¥è¯†å¤„ç†æµç¨‹å¼‚å¸¸ï¼š{str(e)}")
        filtered_context = "ï¼ˆçŸ¥è¯†åº“å¤„ç†å¤±è´¥ï¼‰"

    # æ³¨ï¼šä¸å†å•ç‹¬è¾“å‡ºçŸ¥è¯†åº“å†…å®¹ï¼Œé¿å…ä¸æœ€ç»ˆæç¤ºè¯ä¸­çš„å†…å®¹é‡å¤
    # çŸ¥è¯†åº“å†…å®¹å·²åŒ…å«åœ¨æœ€ç»ˆæç¤ºè¯çš„"çŸ¥è¯†åº“å‚è€ƒ"éƒ¨åˆ†

    # è¿”å›æœ€ç»ˆæç¤ºè¯
    if progress_callback:
        current_step += 1
        progress_callback(progress_steps[current_step][0], progress_steps[current_step][1])

    # æ·»åŠ å»¶æ—¶ï¼Œè®©ç”¨æˆ·èƒ½çœ‹åˆ°è¿›åº¦å˜åŒ–
    time.sleep(1)

    final_prompt = safe_format(
        next_chapter_draft_prompt,
        user_guidance=user_guidance if user_guidance else "æ— ç‰¹æ®ŠæŒ‡å¯¼",
        global_summary=global_summary_text,
        previous_chapter_excerpt=previous_excerpt,
        character_state=character_state_text,
        short_summary=short_summary,
        plot_arcs=plot_arcs_text if plot_arcs_text else "ï¼ˆæ— å‰§æƒ…è¦ç‚¹ï¼‰",
        novel_number=novel_number,
        chapter_title=chapter_title,
        chapter_role=chapter_role,
        chapter_purpose=chapter_purpose,
        suspense_level=suspense_level,
        foreshadowing=foreshadowing,
        plot_twist_level=plot_twist_level,
        surface_cultivation=surface_cultivation,
        actual_cultivation=actual_cultivation,
        chapter_summary=chapter_summary,
        word_number=word_number,
        characters_involved=characters_involved,
        key_items=key_items,
        scene_location=scene_location,
        time_constraint=time_constraint,
        next_chapter_number=next_chapter_number,
        next_chapter_title=next_chapter_title,
        next_chapter_role=next_chapter_role,
        next_chapter_purpose=next_chapter_purpose,
        next_chapter_suspense_level=next_chapter_suspense,
        next_chapter_foreshadowing=next_chapter_foreshadow,
        next_chapter_plot_twist_level=next_chapter_twist,
        next_surface_cultivation=next_surface_cultivation,
        next_actual_cultivation=next_actual_cultivation,
        next_scene_location=next_scene_location,
        next_chapter_summary=next_chapter_summary,
        filtered_context=filtered_context,
        unit_info=unit_info_text
    )

    if prompt_callback:
        prompt_callback(f"\n[å®Œæ•´æç¤ºè¯]\n{final_prompt}")

    if progress_callback:
        progress_callback(1.0, "æç¤ºè¯æ„å»ºå®Œæˆ")

    # æ·»åŠ å»¶æ—¶ï¼Œè®©ç”¨æˆ·èƒ½çœ‹åˆ°æœ€ç»ˆçŠ¶æ€
    time.sleep(1)

    return final_prompt

def generate_chapter_draft(
    api_key: str,
    base_url: str,
    model_name: str, 
    filepath: str,
    novel_number: int,
    word_number: int,
    temperature: float,
    user_guidance: str,
    characters_involved: str,
    key_items: str,
    scene_location: str,
    time_constraint: str,
    embedding_api_key: str,
    embedding_url: str,
    embedding_interface_format: str,
    embedding_model_name: str,
    embedding_retrieval_k: int = 2,
    interface_format: str = "openai",
    max_tokens: int = 2048,
    timeout: int = 600,
    custom_prompt_text: str = None,
    log_func=None
) -> str:
    """
    ç”Ÿæˆç« èŠ‚è‰ç¨¿ï¼Œæ”¯æŒè‡ªå®šä¹‰æç¤ºè¯
    
    å‚æ•°:
        log_func: å¯é€‰çš„æ—¥å¿—å‡½æ•°ï¼Œç”¨äºå°†æ—¥å¿—è¾“å‡ºåˆ°UIã€‚å¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨loggingæ¨¡å—ã€‚
    """
    def log(message):
        if log_func:
            log_func(message)
        else:
            logging.info(message)

    log("=" * 60)
    log(f"ğŸ“– å¼€å§‹ç”Ÿæˆç¬¬{novel_number}ç« è‰ç¨¿...")
    log(f"ğŸ“‚ å°è¯´è·¯å¾„: {filepath}")
    log(f"ğŸ“„ ç›®æ ‡å­—æ•°: {word_number}å­—")
    log("=" * 60)

    # æ­¥éª¤1: å‡†å¤‡æç¤ºè¯
    log("ğŸ“‹ æ­¥éª¤1/3: å‡†å¤‡æç¤ºè¯")

    if custom_prompt_text is None:
        prompt_text = build_chapter_prompt(
            api_key=api_key,
            base_url=base_url,
            model_name=model_name,
            filepath=filepath,
            novel_number=novel_number,
            word_number=word_number,
            temperature=temperature,
            user_guidance=user_guidance,
            characters_involved=characters_involved,
            key_items=key_items,
            scene_location=scene_location,
            time_constraint=time_constraint,
            embedding_api_key=embedding_api_key,
            embedding_url=embedding_url,
            embedding_interface_format=embedding_interface_format,
            embedding_model_name=embedding_model_name,
            embedding_retrieval_k=embedding_retrieval_k,
            interface_format=interface_format,
            max_tokens=max_tokens,
            timeout=timeout
        )
    else:
        prompt_text = custom_prompt_text
    log(f"âœ“ æç¤ºè¯å‡†å¤‡å®Œæˆï¼ˆå…±{len(prompt_text)}å­—ï¼‰")

    chapters_dir = os.path.join(filepath, "chapters")
    os.makedirs(chapters_dir, exist_ok=True)

    # æ­¥éª¤2: åˆ›å»ºLLMé€‚é…å™¨
    log("ğŸ“‹ æ­¥éª¤2/3: åˆ›å»ºLLMé€‚é…å™¨")
    llm_adapter = create_llm_adapter(
        interface_format=interface_format,
        base_url=base_url,
        model_name=model_name,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout
    )
    log("âœ“ LLMé€‚é…å™¨åˆ›å»ºæˆåŠŸ")

    # æ­¥éª¤3: ç”Ÿæˆç« èŠ‚å†…å®¹
    log("ğŸ“‹ æ­¥éª¤3/3: ç”Ÿæˆç« èŠ‚å†…å®¹")
    log("ğŸ“ æ­£åœ¨è°ƒç”¨LLMç”Ÿæˆç« èŠ‚å†…å®¹...")
    chapter_content = invoke_with_cleaning(llm_adapter, prompt_text)
    if not chapter_content.strip():
        log("âš ï¸ ç« èŠ‚å†…å®¹ä¸ºç©ºï¼Œç”Ÿæˆå¤±è´¥")
        return chapter_content
    log("âœ“ ç« èŠ‚å†…å®¹ç”ŸæˆæˆåŠŸ")
    chapter_file = os.path.join(chapters_dir, f"chapter_{novel_number}.txt")
    log("ğŸ’¾ æ­£åœ¨ä¿å­˜ç« èŠ‚å†…å®¹...")
    clear_file_content(chapter_file)
    save_string_to_txt(chapter_content, chapter_file)
    log("âœ“ ç« èŠ‚å†…å®¹ä¿å­˜æˆåŠŸ")
    logging.info(f"[Draft] Chapter {novel_number} generated as a draft.")

    log(f"âœ… ç¬¬{novel_number}ç« è‰ç¨¿ç”Ÿæˆå®Œæˆ")

def generate_chapter_draft_stream(
    api_key: str,
    base_url: str,
    model_name: str,
    filepath: str,
    novel_number: int,
    word_number: int,
    temperature: float,
    user_guidance: str,
    characters_involved: str,
    key_items: str,
    scene_location: str,
    time_constraint: str,
    embedding_api_key: str,
    embedding_url: str,
    embedding_interface_format: str,
    embedding_model_name: str,
    embedding_retrieval_k: int = 2,
    interface_format: str = "openai",
    max_tokens: int = 2048,
    timeout: int = 600,
    custom_prompt_text: str = None,
    stream_callback: callable = None,
    log_func=None
) -> str:
    """
    ç”Ÿæˆç« èŠ‚è‰ç¨¿ï¼Œæ”¯æŒæµå¼è¾“å‡º
    
    å‚æ•°:
        stream_callback: æµå¼è¾“å‡ºå›è°ƒå‡½æ•°ï¼Œæ¥æ”¶æ¯ä¸ªtoken
    
    è¿”å›:
        å®Œæ•´çš„ç« èŠ‚å†…å®¹
    """
    def log(message):
        if log_func:
            log_func(message)
        else:
            logging.info(message)
    
    log("=" * 60)
    log(f"ğŸ“– å¼€å§‹ç”Ÿæˆç¬¬{novel_number}ç« è‰ç¨¿...")
    log(f"ğŸ“‚ å°è¯´è·¯å¾„: {filepath}")
    log(f"ğŸ“„ ç›®æ ‡å­—æ•°: {word_number}å­—")
    log("=" * 60)
    
    # æ­¥éª¤1: å‡†å¤‡æç¤ºè¯
    log("ğŸ“‹ æ­¥éª¤1/4: å‡†å¤‡æç¤ºè¯")
    if custom_prompt_text is None:
        prompt_text = build_chapter_prompt(
            api_key=api_key,
            base_url=base_url,
            model_name=model_name,
            filepath=filepath,
            novel_number=novel_number,
            word_number=word_number,
            temperature=temperature,
            user_guidance=user_guidance,
            characters_involved=characters_involved,
            key_items=key_items,
            scene_location=scene_location,
            time_constraint=time_constraint,
            embedding_api_key=embedding_api_key,
            embedding_url=embedding_url,
            embedding_interface_format=embedding_interface_format,
            embedding_model_name=embedding_model_name,
            embedding_retrieval_k=embedding_retrieval_k,
            interface_format=interface_format,
            max_tokens=max_tokens,
            timeout=timeout
        )
    else:
        prompt_text = custom_prompt_text
    log(f"âœ“ æç¤ºè¯å‡†å¤‡å®Œæˆï¼ˆå…±{len(prompt_text)}å­—ï¼‰")

    chapters_dir = os.path.join(filepath, "chapters")
    os.makedirs(chapters_dir, exist_ok=True)

    # æ­¥éª¤2: åˆ›å»ºLLMé€‚é…å™¨
    log("ğŸ“‹ æ­¥éª¤2/4: åˆ›å»ºLLMé€‚é…å™¨")
    llm_adapter = create_llm_adapter(
        interface_format=interface_format,
        base_url=base_url,
        model_name=model_name,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout
    )
    log("âœ“ LLMé€‚é…å™¨åˆ›å»ºæˆåŠŸ")

    # æ­¥éª¤3: ç”Ÿæˆç« èŠ‚å†…å®¹
    log("ğŸ“‹ æ­¥éª¤3/4: ç”Ÿæˆç« èŠ‚å†…å®¹")
    log("ğŸ“ æ­£åœ¨ç”Ÿæˆç« èŠ‚å†…å®¹ï¼ˆæµå¼è¾“å‡ºï¼‰...")
    # ä½¿ç”¨æµå¼è¾“å‡º
    from novel_generator.stream_utils import invoke_with_cleaning_stream
    chapter_content = invoke_with_cleaning_stream(llm_adapter, prompt_text, stream_callback)
    
    if not chapter_content.strip():
        log("âš ï¸ ç« èŠ‚å†…å®¹ä¸ºç©ºï¼Œç”Ÿæˆå¤±è´¥")
        return chapter_content
    log("âœ“ ç« èŠ‚å†…å®¹ç”ŸæˆæˆåŠŸ")

    # æ­¥éª¤4: ä¿å­˜ç« èŠ‚å†…å®¹
    log("ğŸ“‹ æ­¥éª¤4/4: ä¿å­˜ç« èŠ‚å†…å®¹")
    chapter_file = os.path.join(chapters_dir, f"chapter_{novel_number}.txt")
    log("ğŸ’¾ æ­£åœ¨ä¿å­˜ç« èŠ‚å†…å®¹...")
    clear_file_content(chapter_file)
    save_string_to_txt(chapter_content, chapter_file)
    log("âœ“ ç« èŠ‚å†…å®¹ä¿å­˜æˆåŠŸ")
    logging.info(f"[Draft] Chapter {novel_number} generated as a draft.")
    
    log(f"âœ… ç¬¬{novel_number}ç« è‰ç¨¿ç”Ÿæˆå®Œæˆ")

    

    
    return chapter_content

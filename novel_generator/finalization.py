#novel_generator/finalization.py
# -*- coding: utf-8 -*-
"""
å®šç¨¿ç« èŠ‚å’Œæ‰©å†™ç« èŠ‚ï¼ˆfinalize_chapterã€enrich_chapter_textï¼‰
"""
import os
import logging
from llm_adapters import create_llm_adapter
from embedding_adapters import create_embedding_adapter
from prompt_definitions import summary_prompt, update_character_state_prompt, update_plot_arcs_prompt
from novel_generator.common import invoke_with_cleaning
from utils import read_file, clear_file_content, save_string_to_txt
from novel_generator.vectorstore_utils import update_vector_store

def finalize_chapter(
    novel_number: int,
    word_number: int,
    api_key: str,
    base_url: str,
    model_name: str,
    temperature: float,
    filepath: str,
    embedding_api_key: str,
    embedding_url: str,
    embedding_interface_format: str,
    embedding_model_name: str,
    interface_format: str,
    max_tokens: int,
    timeout: int = 600,
    log_func=None
):
    """
    å¯¹æŒ‡å®šç« èŠ‚åšæœ€ç»ˆå¤„ç†ï¼šæ›´æ–°å‰æ–‡æ‘˜è¦ã€æ›´æ–°è§’è‰²çŠ¶æ€ã€æ’å…¥å‘é‡åº“ç­‰ã€‚
    é»˜è®¤æ— éœ€å†åšæ‰©å†™æ“ä½œï¼Œè‹¥æœ‰éœ€è¦å¯åœ¨å¤–éƒ¨è°ƒç”¨ enrich_chapter_text å¤„ç†åå†å®šç¨¿ã€‚
    
    å‚æ•°:
        log_func: å¯é€‰çš„æ—¥å¿—å‡½æ•°ï¼Œç”¨äºå°†æ—¥å¿—è¾“å‡ºåˆ°UIã€‚å¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨loggingæ¨¡å—ã€‚
    """
    def log(message):
        if log_func:
            log_func(message)
        else:
            logging.info(message)
    
    log("=" * 60)
    log(f"ğŸ“– å¼€å§‹å®šç¨¿ç¬¬{novel_number}ç« ...")
    log(f"ğŸ“‚ å°è¯´è·¯å¾„: {filepath}")
    log(f"ğŸ“„ ç›®æ ‡å­—æ•°: {word_number}å­—")
    log("=" * 60)
    
    # æ­¥éª¤1: è¯»å–ç« èŠ‚å†…å®¹
    log("ğŸ“‹ æ­¥éª¤1/7: è¯»å–ç« èŠ‚å†…å®¹")
    chapters_dir = os.path.join(filepath, "chapters")
    chapter_file = os.path.join(chapters_dir, f"chapter_{novel_number}.txt")
    log(f"ğŸ“„ ç« èŠ‚æ–‡ä»¶: {chapter_file}")
    
    try:
        chapter_text = read_file(chapter_file).strip()
        if not chapter_text:
            log(f"âš ï¸ ç¬¬{novel_number}ç« å†…å®¹ä¸ºç©ºï¼Œæ— æ³•å®šç¨¿")
            return
        log(f"âœ“ å·²è¯»å–ç¬¬{novel_number}ç« å†…å®¹ï¼ˆå…±{len(chapter_text)}å­—ï¼‰")
    except Exception as e:
        log(f"âŒ è¯»å–ç« èŠ‚æ–‡ä»¶å¤±è´¥: {e}")
        return

    # æ­¥éª¤2: è¯»å–ç°æœ‰æ‘˜è¦å’Œè§’è‰²çŠ¶æ€
    log("ğŸ“‹ æ­¥éª¤2/7: è¯»å–ç°æœ‰æ‘˜è¦å’Œè§’è‰²çŠ¶æ€")
    global_summary_file = os.path.join(filepath, "global_summary.txt")
    log(f"ğŸ“„ æ‘˜è¦æ–‡ä»¶: {global_summary_file}")
    old_global_summary = read_file(global_summary_file)
    log(f"âœ“ åŸæ‘˜è¦é•¿åº¦: {len(old_global_summary)}å­—")
    
    character_state_file = os.path.join(filepath, "character_state.txt")
    log(f"ğŸ“„ è§’è‰²çŠ¶æ€æ–‡ä»¶: {character_state_file}")
    old_character_state = read_file(character_state_file)
    log(f"âœ“ åŸè§’è‰²çŠ¶æ€é•¿åº¦: {len(old_character_state)}å­—")

    # æ­¥éª¤3: åˆ›å»ºLLMé€‚é…å™¨
    log("ğŸ“‹ æ­¥éª¤3/7: åˆ›å»ºLLMé€‚é…å™¨")
    try:
        llm_adapter = create_llm_adapter(
            interface_format=interface_format,
            base_url=base_url,
            model_name=model_name,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )
        log("âœ“ LLMé€‚é…å™¨åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        log(f"âŒ LLMé€‚é…å™¨åˆ›å»ºå¤±è´¥: {e}")
        return

    # æ­¥éª¤4: æ›´æ–°å‰æ–‡æ‘˜è¦
    log("ğŸ“‹ æ­¥éª¤4/7: æ›´æ–°å‰æ–‡æ‘˜è¦")
    log("ğŸ“ æ­£åœ¨ç”Ÿæˆæ–°çš„å‰æ–‡æ‘˜è¦...")
    try:
        prompt_summary = summary_prompt.format(
            chapter_text=chapter_text,
            global_summary=old_global_summary
        )
        log(f"ğŸ“ æ‘˜è¦æç¤ºè¯é•¿åº¦: {len(prompt_summary)}å­—")
        new_global_summary = invoke_with_cleaning(llm_adapter, prompt_summary)
        if not new_global_summary.strip():
            new_global_summary = old_global_summary
            log("âš ï¸ å‰æ–‡æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä¿ç•™åŸæ‘˜è¦")
        else:
            log(f"âœ“ æ–°å‰æ–‡æ‘˜è¦ç”ŸæˆæˆåŠŸï¼ˆå…±{len(new_global_summary)}å­—ï¼‰")
            log(f"ğŸ“Š æ‘˜è¦å˜åŒ–: {len(new_global_summary) - len(old_global_summary):+d}å­—")
    except Exception as e:
        log(f"âŒ æ›´æ–°å‰æ–‡æ‘˜è¦æ—¶å‡ºé”™: {e}")
        new_global_summary = old_global_summary
        log("âš ï¸ ä½¿ç”¨åŸæ‘˜è¦ç»§ç»­æµç¨‹")

    # æ­¥éª¤5: æ›´æ–°è§’è‰²çŠ¶æ€
    log("ğŸ“‹ æ­¥éª¤5/7: æ›´æ–°è§’è‰²çŠ¶æ€")
    log("ğŸ‘¤ æ­£åœ¨æ›´æ–°è§’è‰²çŠ¶æ€...")
    try:
        prompt_char_state = update_character_state_prompt.format(
            chapter_text=chapter_text,
            old_state=old_character_state
        )
        log(f"ğŸ“ è§’è‰²çŠ¶æ€æç¤ºè¯é•¿åº¦: {len(prompt_char_state)}å­—")
        new_char_state = invoke_with_cleaning(llm_adapter, prompt_char_state)
        if not new_char_state.strip():
            new_char_state = old_character_state
            log("âš ï¸ è§’è‰²çŠ¶æ€æ›´æ–°å¤±è´¥ï¼Œä¿ç•™åŸçŠ¶æ€")
        else:
            log(f"âœ“ æ–°è§’è‰²çŠ¶æ€ç”ŸæˆæˆåŠŸï¼ˆå…±{len(new_char_state)}å­—ï¼‰")
            log(f"ğŸ“Š è§’è‰²çŠ¶æ€å˜åŒ–: {len(new_char_state) - len(old_character_state):+d}å­—")
            # ç»Ÿè®¡è§’è‰²æ•°é‡
            role_count = new_char_state.count("ï¼š")
            log(f"ğŸ‘¥ å½“å‰è®°å½•è§’è‰²æ•°é‡: {role_count}ä¸ª")
    except Exception as e:
        log(f"âŒ æ›´æ–°è§’è‰²çŠ¶æ€æ—¶å‡ºé”™: {e}")
        new_char_state = old_character_state
        log("âš ï¸ ä½¿ç”¨åŸè§’è‰²çŠ¶æ€ç»§ç»­æµç¨‹")

    # æ­¥éª¤6: æ›´æ–°å‰§æƒ…è¦ç‚¹å’Œæœªè§£å†³å†²çª
    log("ğŸ“‹ æ­¥éª¤6/7: æ›´æ–°å‰§æƒ…è¦ç‚¹å’Œæœªè§£å†³å†²çª")
    log("ğŸ“Š æ­£åœ¨æ›´æ–°å‰§æƒ…è¦ç‚¹å’Œæœªè§£å†³å†²çªè®°å½•...")
    plot_arcs_file = os.path.join(filepath, "plot_arcs.txt")
    log(f"ğŸ“„ å‰§æƒ…è¦ç‚¹æ–‡ä»¶: {plot_arcs_file}")
    old_plot_arcs = ""
    if os.path.exists(plot_arcs_file):
        old_plot_arcs = read_file(plot_arcs_file)
        log(f"âœ“ åŸå‰§æƒ…è¦ç‚¹é•¿åº¦: {len(old_plot_arcs)}å­—")
    try:
        prompt_plot_arcs = update_plot_arcs_prompt.format(
            chapter_text=chapter_text,
            old_plot_arcs=old_plot_arcs
        )
        log(f"ğŸ“ å‰§æƒ…è¦ç‚¹æç¤ºè¯é•¿åº¦: {len(prompt_plot_arcs)}å­—")
        new_plot_arcs = invoke_with_cleaning(llm_adapter, prompt_plot_arcs)
        if not new_plot_arcs.strip():
            new_plot_arcs = old_plot_arcs
            log("âš ï¸ å‰§æƒ…è¦ç‚¹å’Œæœªè§£å†³å†²çªæ›´æ–°å¤±è´¥ï¼Œä¿ç•™åŸè®°å½•")
        else:
            log(f"âœ“ æ–°å‰§æƒ…è¦ç‚¹ç”ŸæˆæˆåŠŸï¼ˆå…±{len(new_plot_arcs)}å­—ï¼‰")
            log(f"ğŸ“Š å‰§æƒ…è¦ç‚¹å˜åŒ–: {len(new_plot_arcs) - len(old_plot_arcs):+d}å­—")
    except Exception as e:
        log(f"âŒ æ›´æ–°å‰§æƒ…è¦ç‚¹æ—¶å‡ºé”™: {e}")
        new_plot_arcs = old_plot_arcs
        log("âš ï¸ ä½¿ç”¨åŸå‰§æƒ…è¦ç‚¹ç»§ç»­æµç¨‹")

    # ç»Ÿè®¡æœªè§£å†³å†²çªæ•°é‡
    unresolved_conflicts = new_plot_arcs.count("æœªè§£å†³")
    log(f"âœ“ å‰§æƒ…è¦ç‚¹å·²æ›´æ–°ï¼ˆå…±{len(new_plot_arcs)}å­—ï¼ŒåŒ…å«{unresolved_conflicts}ä¸ªæœªè§£å†³å†²çªï¼‰")

    clear_file_content(global_summary_file)
    save_string_to_txt(new_global_summary, global_summary_file)
    clear_file_content(character_state_file)
    save_string_to_txt(new_char_state, character_state_file)
    clear_file_content(plot_arcs_file)
    save_string_to_txt(new_plot_arcs, plot_arcs_file)
    
    # åŒæ­¥è§’è‰²åº“
    log("ğŸ‘¥ æ­£åœ¨åŒæ­¥è§’è‰²åº“...")
    try:
        _sync_character_library(filepath, new_char_state)
        log("âœ“ è§’è‰²åº“åŒæ­¥å®Œæˆ")
    except Exception as e:
        log(f"âŒ åŒæ­¥è§’è‰²åº“æ—¶å‡ºé”™: {e}")
        log("âš ï¸ è§’è‰²åº“åŒæ­¥å¤±è´¥ï¼Œä½†ç»§ç»­æµç¨‹")

    # æ­¥éª¤7: æ›´æ–°å‘é‡åº“
    log("ğŸ“‹ æ­¥éª¤7/7: æ›´æ–°å‘é‡åº“")
    log("ğŸ” æ­£åœ¨æ›´æ–°å‘é‡åº“...")
    try:
        updated_count = update_vector_store(
            embedding_adapter=create_embedding_adapter(
                embedding_interface_format,
                embedding_api_key,
                embedding_url,
                embedding_model_name
            ),
            new_chapter=chapter_text,
            filepath=filepath
        )
        if updated_count > 0:
            log(f"âœ“ å‘é‡åº“æ›´æ–°æˆåŠŸï¼Œæœ¬æ¬¡æ›´æ–°{updated_count}æ¡æ•°æ®")
        else:
            log("âš ï¸ å‘é‡åº“æ›´æ–°å¤±è´¥æˆ–æ— æ•°æ®æ›´æ–°")
    except Exception as e:
        log(f"âŒ æ›´æ–°å‘é‡åº“æ—¶å‡ºé”™: {e}")
        log("âš ï¸ å‘é‡åº“æ›´æ–°å¤±è´¥ï¼Œä½†ç»§ç»­æµç¨‹")

def enrich_chapter_text(
    chapter_text: str,
    word_number: int,
    api_key: str,
    base_url: str,
    model_name: str,
    temperature: float,
    interface_format: str,
    max_tokens: int,
    timeout: int=600
) -> str:
    """
    å¯¹ç« èŠ‚æ–‡æœ¬è¿›è¡Œæ‰©å†™ï¼Œä½¿å…¶æ›´æ¥è¿‘ word_number å­—æ•°ï¼Œä¿æŒå‰§æƒ…è¿è´¯ã€‚
    """
    llm_adapter = create_llm_adapter(
        interface_format=interface_format,
        base_url=base_url,
        model_name=model_name,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout
    )
    prompt = f"""ä»¥ä¸‹ç« èŠ‚æ–‡æœ¬è¾ƒçŸ­ï¼Œè¯·åœ¨ä¿æŒå‰§æƒ…è¿è´¯çš„å‰æä¸‹è¿›è¡Œæ‰©å†™ï¼Œä½¿å…¶æ›´å……å®ï¼Œæ¥è¿‘ {word_number} å­—å·¦å³ï¼š
åŸå†…å®¹ï¼š
{chapter_text}
"""
    enriched_text = invoke_with_cleaning(llm_adapter, prompt)
    return enriched_text


def _sync_character_library(filepath: str, character_state: str):
    """
    å°†è§’è‰²çŠ¶æ€åŒæ­¥åˆ°è§’è‰²åº“
    """
    import re
    
    # è§’è‰²åº“è·¯å¾„
    library_path = os.path.join(filepath, "è§’è‰²åº“")
    os.makedirs(library_path, exist_ok=True)
    
    # ç¡®ä¿"å…¨éƒ¨"åˆ†ç±»å­˜åœ¨
    all_category = os.path.join(library_path, "å…¨éƒ¨")
    os.makedirs(all_category, exist_ok=True)
    
    # è§£æè§’è‰²çŠ¶æ€
    characters = _parse_character_state(character_state)
    
    # æ›´æ–°æˆ–åˆ›å»ºè§’è‰²æ–‡ä»¶
    for char_name, char_data in characters.items():
        char_file = os.path.join(all_category, f"{char_name}.txt")
        
        # æ„å»ºè§’è‰²æ–‡ä»¶å†…å®¹
        content_lines = [f"{char_name}ï¼š"]
        for attr_name, items in char_data.items():
            content_lines.append(f"â”œâ”€â”€{attr_name}")
            for i, item in enumerate(items):
                prefix = "â”œâ”€â”€" if i < len(items) - 1 else "â””â”€â”€"
                content_lines.append(f"â”‚  {prefix}{item}")
        
        # å†™å…¥æ–‡ä»¶
        with open(char_file, "w", encoding="utf-8") as f:
            f.write("\n".join(content_lines))


def _parse_character_state(character_state: str) -> dict:
    """
    è§£æè§’è‰²çŠ¶æ€æ–‡æœ¬ï¼Œè¿”å›è§’è‰²å­—å…¸
    """
    import re
    
    characters = {}
    current_char = None
    current_attr = None
    
    for line in character_state.split("\n"):
        # ä¸å¯¹è¡Œè¿›è¡Œstripï¼Œä»¥ä¿ç•™â”‚å‰ç¼€
        original_line = line
        line = line.strip()
        
        # æ£€æµ‹è§’è‰²åç§°è¡Œï¼ˆå…¼å®¹ä¸­è‹±æ–‡å†’å·å’Œå‰åç©ºæ ¼ï¼‰
        role_match = re.match(r"^([\u4e00-\u9fa5a-zA-Z0-9]+)\s*[:ï¼š]\s*$", line)
        if role_match:
            current_char = role_match.group(1).strip()
            characters[current_char] = {
                "ç‰©å“": [],
                "èƒ½åŠ›": [],
                "çŠ¶æ€": [],
                "ä¸»è¦è§’è‰²é—´å…³ç³»ç½‘": [],
                "è§¦å‘æˆ–åŠ æ·±çš„äº‹ä»¶": []
            }
            current_attr = None
            continue
        
        if not current_char:
            continue
        
        # è§£æå±æ€§ï¼ˆæ”¯æŒå­å±æ€§ï¼‰
        # å…ˆå°è¯•åŒ¹é…å¸¦â”‚å‰ç¼€çš„æ ¼å¼ï¼ˆå¸¦æˆ–ä¸å¸¦å†’å·ï¼‰
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œç¡®ä¿åªåŒ¹é…å±æ€§åç§°ï¼Œä¸åŒ¹é…æ¡ç›®
        attr_match = re.match(r"^â”‚\s+([â”œâ””]â”€â”€)([^ï¼š:ï¼š]+)\s*[:ï¼š]?$", original_line)
        if not attr_match:
            # å†å°è¯•åŒ¹é…ä¸å¸¦â”‚å‰ç¼€çš„æ ¼å¼ï¼ˆå¸¦æˆ–ä¸å¸¦å†’å·ï¼‰
            attr_match = re.match(r"^([â”œâ””]â”€â”€)([^ï¼š:ï¼š]+)\s*[:ï¼š]?$", original_line)
        if attr_match:
            prefix, attr_name = attr_match.groups()
            attr_name = attr_name.strip()
            # åŒ¹é…é¢„è®¾å±æ€§
            for preset_attr in characters[current_char]:
                if attr_name == preset_attr:
                    current_attr = preset_attr
                    break
            continue
        
        # è§£æå±æ€§æ¡ç›® - æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        # 1. ä»¥â”‚å¼€å¤´çš„æ¡ç›®ï¼ˆæ ‡å‡†æ ¼å¼ï¼‰
        # 2. ç›´æ¥ä»¥â”œâ”€â”€æˆ–â””â”€â”€å¼€å¤´çš„æ¡ç›®ï¼ˆéæ ‡å‡†æ ¼å¼ï¼‰
        # æ³¨æ„ï¼šå¿…é¡»ç¡®ä¿ä¸å°†å±æ€§åˆ†ç±»è¡Œè¯¯è¯†åˆ«ä¸ºæ¡ç›®
        item_match = re.match(r"^â”‚\s+([â”œâ””]â”€â”€)\s*(.*)", original_line)
        if item_match and current_attr:
            prefix, content = item_match.groups()
            content = content.strip()
            if content:
                # æ£€æŸ¥å†…å®¹æ˜¯å¦æ˜¯å±æ€§åˆ†ç±»åç§°ï¼ˆé¿å…å°†åˆ†ç±»è¯¯è¯†åˆ«ä¸ºæ¡ç›®ï¼‰
                # åªæœ‰å½“å†…å®¹å®Œå…¨åŒ¹é…å±æ€§åˆ†ç±»åç§°æ—¶æ‰è·³è¿‡
                if content not in ["ç‰©å“", "èƒ½åŠ›", "çŠ¶æ€", "ä¸»è¦è§’è‰²é—´å…³ç³»ç½‘", "è§¦å‘æˆ–åŠ æ·±çš„äº‹ä»¶"]:
                    characters[current_char][current_attr].append(content)
        else:
            # å°è¯•è§£æä¸ä»¥â”‚å¼€å¤´çš„æ¡ç›®
            direct_item_match = re.match(r"^\s+([â”œâ””]â”€â”€)\s*(.*)", original_line)
            if direct_item_match and current_attr:
                prefix, content = direct_item_match.groups()
                content = content.strip()
                if content:
                    # æ£€æŸ¥å†…å®¹æ˜¯å¦æ˜¯å±æ€§åˆ†ç±»åç§°ï¼ˆé¿å…å°†åˆ†ç±»è¯¯è¯†åˆ«ä¸ºæ¡ç›®ï¼‰
                    if content not in ["ç‰©å“", "èƒ½åŠ›", "çŠ¶æ€", "ä¸»è¦è§’è‰²é—´å…³ç³»ç½‘", "è§¦å‘æˆ–åŠ æ·±çš„äº‹ä»¶"]:
                        characters[current_char][current_attr].append(content)
    
    return characters

#novel_generator/finalization.py
# -*- coding: utf-8 -*-
"""
å®šç¨¿ç« èŠ‚å’Œæ‰©å†™ç« èŠ‚ï¼ˆfinalize_chapterã€enrich_chapter_textï¼‰
"""
import os
import logging
from llm_adapters import create_llm_adapter
from embedding_adapters import create_embedding_adapter
from prompt_definitions import summary_prompt, update_character_state_prompt, update_plot_arcs_prompt
from novel_generator.common import invoke_with_cleaning
from utils import read_file, clear_file_content, save_string_to_txt
from novel_generator.vectorstore_utils import update_vector_store

def finalize_chapter(
    novel_number: int,
    word_number: int,
    api_key: str,
    base_url: str,
    model_name: str,
    temperature: float,
    filepath: str,
    embedding_api_key: str,
    embedding_url: str,
    embedding_interface_format: str,
    embedding_model_name: str,
    interface_format: str,
    max_tokens: int,
    timeout: int = 600
):
    """
    å¯¹æŒ‡å®šç« èŠ‚åšæœ€ç»ˆå¤„ç†ï¼šæ›´æ–°å‰æ–‡æ‘˜è¦ã€æ›´æ–°è§’è‰²çŠ¶æ€ã€æ’å…¥å‘é‡åº“ç­‰ã€‚
    é»˜è®¤æ— éœ€å†åšæ‰©å†™æ“ä½œï¼Œè‹¥æœ‰éœ€è¦å¯åœ¨å¤–éƒ¨è°ƒç”¨ enrich_chapter_text å¤„ç†åå†å®šç¨¿ã€‚
    """
    logging.info(f"ğŸ“– å¼€å§‹å®šç¨¿ç¬¬{novel_number}ç« ...")
    chapters_dir = os.path.join(filepath, "chapters")
    chapter_file = os.path.join(chapters_dir, f"chapter_{novel_number}.txt")
    chapter_text = read_file(chapter_file).strip()
    if not chapter_text:
        logging.warning(f"âš ï¸ ç¬¬{novel_number}ç« å†…å®¹ä¸ºç©ºï¼Œæ— æ³•å®šç¨¿")
        return
    logging.info(f"âœ“ å·²è¯»å–ç¬¬{novel_number}ç« å†…å®¹ï¼ˆå…±{len(chapter_text)}å­—ï¼‰")

    global_summary_file = os.path.join(filepath, "global_summary.txt")
    old_global_summary = read_file(global_summary_file)
    character_state_file = os.path.join(filepath, "character_state.txt")
    old_character_state = read_file(character_state_file)

    llm_adapter = create_llm_adapter(
        interface_format=interface_format,
        base_url=base_url,
        model_name=model_name,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout
    )

    logging.info("ğŸ“ æ­£åœ¨æ›´æ–°å‰æ–‡æ‘˜è¦...")
    prompt_summary = summary_prompt.format(
        chapter_text=chapter_text,
        global_summary=old_global_summary
    )
    new_global_summary = invoke_with_cleaning(llm_adapter, prompt_summary)
    if not new_global_summary.strip():
        new_global_summary = old_global_summary
        logging.warning("âš ï¸ å‰æ–‡æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä¿ç•™åŸæ‘˜è¦")
    logging.info(f"âœ“ å‰æ–‡æ‘˜è¦å·²æ›´æ–°ï¼ˆå…±{len(new_global_summary)}å­—ï¼‰")

    logging.info("ğŸ‘¤ æ­£åœ¨æ›´æ–°è§’è‰²çŠ¶æ€...")
    prompt_char_state = update_character_state_prompt.format(
        chapter_text=chapter_text,
        old_state=old_character_state
    )
    new_char_state = invoke_with_cleaning(llm_adapter, prompt_char_state)
    if not new_char_state.strip():
        new_char_state = old_character_state
        logging.warning("âš ï¸ è§’è‰²çŠ¶æ€æ›´æ–°å¤±è´¥ï¼Œä¿ç•™åŸçŠ¶æ€")
    logging.info(f"âœ“ è§’è‰²çŠ¶æ€å·²æ›´æ–°ï¼ˆå…±{len(new_char_state)}å­—ï¼‰")

    # æ›´æ–°å‰§æƒ…è¦ç‚¹
    plot_arcs_file = os.path.join(filepath, "plot_arcs.txt")
    old_plot_arcs = ""
    if os.path.exists(plot_arcs_file):
        old_plot_arcs = read_file(plot_arcs_file)
    prompt_plot_arcs = update_plot_arcs_prompt.format(
        chapter_text=chapter_text,
        old_plot_arcs=old_plot_arcs
    )
    new_plot_arcs = invoke_with_cleaning(llm_adapter, prompt_plot_arcs)
    if not new_plot_arcs.strip():
        new_plot_arcs = old_plot_arcs

    clear_file_content(global_summary_file)
    save_string_to_txt(new_global_summary, global_summary_file)
    clear_file_content(character_state_file)
    save_string_to_txt(new_char_state, character_state_file)
    clear_file_content(plot_arcs_file)
    save_string_to_txt(new_plot_arcs, plot_arcs_file)
    
    # åŒæ­¥è§’è‰²åº“
    _sync_character_library(filepath, new_char_state)

    logging.info("ğŸ” æ­£åœ¨æ›´æ–°å‘é‡åº“...")
    update_vector_store(
        embedding_adapter=create_embedding_adapter(
            embedding_interface_format,
            embedding_api_key,
            embedding_url,
            embedding_model_name
        ),
        new_chapter=chapter_text,
        filepath=filepath
    )
    logging.info("âœ“ å‘é‡åº“æ›´æ–°å®Œæˆ")

    logging.info(f"âœ… ç¬¬{novel_number}ç« å®šç¨¿å®Œæˆï¼")

def enrich_chapter_text(
    chapter_text: str,
    word_number: int,
    api_key: str,
    base_url: str,
    model_name: str,
    temperature: float,
    interface_format: str,
    max_tokens: int,
    timeout: int=600
) -> str:
    """
    å¯¹ç« èŠ‚æ–‡æœ¬è¿›è¡Œæ‰©å†™ï¼Œä½¿å…¶æ›´æ¥è¿‘ word_number å­—æ•°ï¼Œä¿æŒå‰§æƒ…è¿è´¯ã€‚
    """
    llm_adapter = create_llm_adapter(
        interface_format=interface_format,
        base_url=base_url,
        model_name=model_name,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout
    )
    prompt = f"""ä»¥ä¸‹ç« èŠ‚æ–‡æœ¬è¾ƒçŸ­ï¼Œè¯·åœ¨ä¿æŒå‰§æƒ…è¿è´¯çš„å‰æä¸‹è¿›è¡Œæ‰©å†™ï¼Œä½¿å…¶æ›´å……å®ï¼Œæ¥è¿‘ {word_number} å­—å·¦å³ï¼š
åŸå†…å®¹ï¼š
{chapter_text}
"""
    enriched_text = invoke_with_cleaning(llm_adapter, prompt)
    return enriched_text


def _sync_character_library(filepath: str, character_state: str):
    """
    å°†è§’è‰²çŠ¶æ€åŒæ­¥åˆ°è§’è‰²åº“
    """
    import re
    
    # è§’è‰²åº“è·¯å¾„
    library_path = os.path.join(filepath, "è§’è‰²åº“")
    os.makedirs(library_path, exist_ok=True)
    
    # ç¡®ä¿"å…¨éƒ¨"åˆ†ç±»å­˜åœ¨
    all_category = os.path.join(library_path, "å…¨éƒ¨")
    os.makedirs(all_category, exist_ok=True)
    
    # è§£æè§’è‰²çŠ¶æ€
    characters = _parse_character_state(character_state)
    
    # æ›´æ–°æˆ–åˆ›å»ºè§’è‰²æ–‡ä»¶
    for char_name, char_data in characters.items():
        char_file = os.path.join(all_category, f"{char_name}.txt")
        
        # æ„å»ºè§’è‰²æ–‡ä»¶å†…å®¹
        content_lines = [f"{char_name}ï¼š"]
        for attr_name, items in char_data.items():
            content_lines.append(f"â”œâ”€â”€{attr_name}")
            for i, item in enumerate(items):
                prefix = "â”œâ”€â”€" if i < len(items) - 1 else "â””â”€â”€"
                content_lines.append(f"â”‚  {prefix}{item}")
        
        # å†™å…¥æ–‡ä»¶
        with open(char_file, "w", encoding="utf-8") as f:
            f.write("\n".join(content_lines))


def _parse_character_state(character_state: str) -> dict:
    """
    è§£æè§’è‰²çŠ¶æ€æ–‡æœ¬ï¼Œè¿”å›è§’è‰²å­—å…¸
    """
    import re
    
    characters = {}
    current_char = None
    current_attr = None
    
    for line in character_state.split("\n"):
        # ä¸å¯¹è¡Œè¿›è¡Œstripï¼Œä»¥ä¿ç•™â”‚å‰ç¼€
        original_line = line
        line = line.strip()
        
        # æ£€æµ‹è§’è‰²åç§°è¡Œï¼ˆå…¼å®¹ä¸­è‹±æ–‡å†’å·å’Œå‰åç©ºæ ¼ï¼‰
        role_match = re.match(r"^([\u4e00-\u9fa5a-zA-Z0-9]+)\s*[:ï¼š]\s*$", line)
        if role_match:
            current_char = role_match.group(1).strip()
            characters[current_char] = {
                "ç‰©å“": [],
                "èƒ½åŠ›": [],
                "çŠ¶æ€": [],
                "ä¸»è¦è§’è‰²é—´å…³ç³»ç½‘": [],
                "è§¦å‘æˆ–åŠ æ·±çš„äº‹ä»¶": []
            }
            current_attr = None
            continue
        
        if not current_char:
            continue
        
        # è§£æå±æ€§ï¼ˆæ”¯æŒå­å±æ€§ï¼‰
        # å…ˆå°è¯•åŒ¹é…å¸¦â”‚å‰ç¼€çš„æ ¼å¼ï¼ˆå¸¦æˆ–ä¸å¸¦å†’å·ï¼‰
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œç¡®ä¿åªåŒ¹é…å±æ€§åç§°ï¼Œä¸åŒ¹é…æ¡ç›®
        attr_match = re.match(r"^â”‚\s+([â”œâ””]â”€â”€)([^ï¼š:ï¼š]+)\s*[:ï¼š]?$", original_line)
        if not attr_match:
            # å†å°è¯•åŒ¹é…ä¸å¸¦â”‚å‰ç¼€çš„æ ¼å¼ï¼ˆå¸¦æˆ–ä¸å¸¦å†’å·ï¼‰
            attr_match = re.match(r"^([â”œâ””]â”€â”€)([^ï¼š:ï¼š]+)\s*[:ï¼š]?$", original_line)
        if attr_match:
            prefix, attr_name = attr_match.groups()
            attr_name = attr_name.strip()
            # åŒ¹é…é¢„è®¾å±æ€§
            for preset_attr in characters[current_char]:
                if attr_name == preset_attr:
                    current_attr = preset_attr
                    break
            continue
        
        # è§£æå±æ€§æ¡ç›® - æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        # 1. ä»¥â”‚å¼€å¤´çš„æ¡ç›®ï¼ˆæ ‡å‡†æ ¼å¼ï¼‰
        # 2. ç›´æ¥ä»¥â”œâ”€â”€æˆ–â””â”€â”€å¼€å¤´çš„æ¡ç›®ï¼ˆéæ ‡å‡†æ ¼å¼ï¼‰
        # æ³¨æ„ï¼šå¿…é¡»ç¡®ä¿ä¸å°†å±æ€§åˆ†ç±»è¡Œè¯¯è¯†åˆ«ä¸ºæ¡ç›®
        item_match = re.match(r"^â”‚\s+([â”œâ””]â”€â”€)\s*(.*)", original_line)
        if item_match and current_attr:
            prefix, content = item_match.groups()
            content = content.strip()
            if content:
                # æ£€æŸ¥å†…å®¹æ˜¯å¦æ˜¯å±æ€§åˆ†ç±»åç§°ï¼ˆé¿å…å°†åˆ†ç±»è¯¯è¯†åˆ«ä¸ºæ¡ç›®ï¼‰
                # åªæœ‰å½“å†…å®¹å®Œå…¨åŒ¹é…å±æ€§åˆ†ç±»åç§°æ—¶æ‰è·³è¿‡
                if content not in ["ç‰©å“", "èƒ½åŠ›", "çŠ¶æ€", "ä¸»è¦è§’è‰²é—´å…³ç³»ç½‘", "è§¦å‘æˆ–åŠ æ·±çš„äº‹ä»¶"]:
                    characters[current_char][current_attr].append(content)
        else:
            # å°è¯•è§£æä¸ä»¥â”‚å¼€å¤´çš„æ¡ç›®
            direct_item_match = re.match(r"^\s+([â”œâ””]â”€â”€)\s*(.*)", original_line)
            if direct_item_match and current_attr:
                prefix, content = direct_item_match.groups()
                content = content.strip()
                if content:
                    # æ£€æŸ¥å†…å®¹æ˜¯å¦æ˜¯å±æ€§åˆ†ç±»åç§°ï¼ˆé¿å…å°†åˆ†ç±»è¯¯è¯†åˆ«ä¸ºæ¡ç›®ï¼‰
                    if content not in ["ç‰©å“", "èƒ½åŠ›", "çŠ¶æ€", "ä¸»è¦è§’è‰²é—´å…³ç³»ç½‘", "è§¦å‘æˆ–åŠ æ·±çš„äº‹ä»¶"]:
                        characters[current_char][current_attr].append(content)
    
    return characters
